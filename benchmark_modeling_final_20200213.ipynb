{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Library Import\n",
    "\n",
    "# Basic\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 100)\n",
    "warnings.filterwarnings('ignore')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# AutoML optuna library\n",
    "import optuna\n",
    "import optuna.integration.lightgbm as lgb\n",
    "\n",
    "# Linear Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Tree Model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Metric\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "df = pd.read_csv('original_full_data.csv', index_col = 0)\n",
    "mmdf = pd.read_csv('bloomberg_map_index_20200130.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the date the target data is present ~\n",
    "df = df.loc[9132 :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class generates derived variables & shifting target variable\n",
    "\n",
    "class Builder:\n",
    "\n",
    "    def __init__(self, input_df, map_df, target_day):\n",
    "        self.df = input_df\n",
    "        self.mdf = map_df\n",
    "        self.lag_days = [1, 5, 20, 60, 120, 260] # lagging day variation [1day 1week 1month 3month 6month 1year]\n",
    "        self.target_day = target_day # ex) 1, 260\n",
    "\n",
    "\n",
    "    # shifting function\n",
    "\n",
    "    def targeter(self, target_column):\n",
    "        self.df[target_column] = self.df[target_column].shift(-1*int(self.target_day))\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    # generating derived columns function : return ex) 20060107/20060101\n",
    "\n",
    "    def returner(self, target_column):      \n",
    "\n",
    "        col_name =  []\n",
    "        # creating new column name\n",
    "        for i in self.lag_days:\n",
    "            col_name.append(target_column + str(i))\n",
    "            \n",
    "        # appending lagged columns\n",
    "        for i, j in enumerate(self.lag_days):\n",
    "            self.df[locals()['col_name'][i]] = self.df[target_column] / self.df[target_column].shift(periods = j, axis = 0)\n",
    "\n",
    "        return self.df \n",
    "\n",
    "    # generating derived function : minus ex) 20060107 - 20060101\n",
    "    \n",
    "    def differ(self, target_column):\n",
    "\n",
    "        col_name = []\n",
    "        for i in self.lag_days:\n",
    "            col_name.append(target_column + str(i))\n",
    "        for i, j in enumerate(self.lag_days):\n",
    "            self.df[locals()['col_name'][i]] = self.df[target_column] - self.df[target_column].shift(periods = j, axis = 0)\n",
    "        return self.df\n",
    "\n",
    "    # generating derived function : volume * close  -> convert to return \n",
    "\n",
    "    def producter(self, target_column):\n",
    "\n",
    "        col_name = (str(target_column).split('_'))[0]       \n",
    "        self.df[locals()['col_name'] + '_product'] = self.df[col_name + '_volume'] * self.df[col_name + '_close'] # j volume과 j close를 곱해서 \n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    # executing functions\n",
    "    \n",
    "    def execution(self):\n",
    "        for i, j in enumerate(self.mdf['ticker']): # mdf에서 ticker 확인\n",
    "            if j in self.df.columns:\n",
    "                if self.mdf.loc[i, :][1] == 'product':  # key값이 product인 경우\n",
    "                    self.producter(j)\n",
    "                    col = str(j).split('_')[0]\n",
    "                    self.df[col + '_product'] = self.df[col + '_product'] / self.df[col + '_product'].shift(1)\n",
    "                    \n",
    "                #elif self.mdf.loc[i, :][1] == 'minus': # key값이 minus인 경우\n",
    "                #    self.differ(j)\n",
    "\n",
    "                #elif self.mdf.loc[i, :][1] == 'change': # key값이 change인 경우\n",
    "                #    self.returner(j)\n",
    "\n",
    "\n",
    "                elif self.mdf.loc[i, :][1] == 'target': # key값이 target인 경우 (현재 KRXsemiconductor_change)\n",
    "                    self.targeter(j)\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass\n",
    "                            \n",
    "        return self.df\n",
    "\n",
    "\n",
    "    # side : creating auto correlation graph function\n",
    "\n",
    "    def acf_cal(self, target):\n",
    "        data = pd.DataFrame(self.df[target])\n",
    "        data = data.interpolate(method = 'cubic', limit_area = 'inside').fillna(method = 'ffill').fillna(method = 'bfill')\n",
    "        acf = sm.tsa.acf(data)      \n",
    "        plt.stem(acf)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date preprocessing\n",
    "\n",
    "newdf['date'] = pd.to_datetime(newdf['date'])\n",
    "newdf.set_index('date', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna cubic -> ffill -> bfill\n",
    "\n",
    "newdf = newdf.interpolate(method = 'cubic', limit_area = 'inside')\n",
    "newdf = newdf.fillna(method = 'ffill')\n",
    "newdf = newdf.fillna(method = 'bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check na value\n",
    "\n",
    "newdf.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class load\n",
    "\n",
    "cla = Builder(newdf, mmdf, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDPCQOQIndex</th>\n",
       "      <th>GDPCYOYIndex</th>\n",
       "      <th>GDPCTOT%Index</th>\n",
       "      <th>GPDITOC%Index</th>\n",
       "      <th>GPGSTOC%Index</th>\n",
       "      <th>RGCDCIPIIndex</th>\n",
       "      <th>GDPCUR$Index</th>\n",
       "      <th>GDPCURYIndex</th>\n",
       "      <th>GDPPIQQIndex</th>\n",
       "      <th>GDPCPCECIndex</th>\n",
       "      <th>CPIYOYIndex</th>\n",
       "      <th>CPIXYOYIndex</th>\n",
       "      <th>PCEDEFYIndex</th>\n",
       "      <th>PCECYOYIndex</th>\n",
       "      <th>CPICHNGIndex</th>\n",
       "      <th>CPUPXCHGIndex</th>\n",
       "      <th>FDIDFDMOIndex</th>\n",
       "      <th>FDIDSGMOIndex</th>\n",
       "      <th>FDIUFDYOIndex</th>\n",
       "      <th>FDIUSGYOIndex</th>\n",
       "      <th>USURTOTIndex</th>\n",
       "      <th>USUDMAERIndex</th>\n",
       "      <th>INJCJCIndex</th>\n",
       "      <th>INJCSPIndex</th>\n",
       "      <th>NFPTCHIndex</th>\n",
       "      <th>...</th>\n",
       "      <th>AMD_product</th>\n",
       "      <th>APPLE_product</th>\n",
       "      <th>AppliedMaterials_product</th>\n",
       "      <th>Aspeed_product</th>\n",
       "      <th>Dell_product</th>\n",
       "      <th>Facebook_product</th>\n",
       "      <th>Google_product</th>\n",
       "      <th>HPE_product</th>\n",
       "      <th>Intel_product</th>\n",
       "      <th>LamResearch_product</th>\n",
       "      <th>MediaTek_product</th>\n",
       "      <th>Micron_product</th>\n",
       "      <th>Microsoft_product</th>\n",
       "      <th>Nuvoton_product</th>\n",
       "      <th>Nvidia_product</th>\n",
       "      <th>Philadelphia_product</th>\n",
       "      <th>QCOM_product</th>\n",
       "      <th>SamsungElectronics_product</th>\n",
       "      <th>ShinEtsuChemical_product</th>\n",
       "      <th>Siltronic_product</th>\n",
       "      <th>SKhynix_product</th>\n",
       "      <th>Sumco_product</th>\n",
       "      <th>TokyoElectron_product</th>\n",
       "      <th>TSMC_product</th>\n",
       "      <th>UMC_product</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-01-02</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>102.7</td>\n",
       "      <td>13603.93</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.06622</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-03</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>102.7</td>\n",
       "      <td>13603.93</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.06622</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862009</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.048766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-04</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>102.7</td>\n",
       "      <td>13603.93</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.06622</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.682030</td>\n",
       "      <td>0.771197</td>\n",
       "      <td>1.026287</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.223552</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.829714</td>\n",
       "      <td>0.967572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.380558</td>\n",
       "      <td>0.743634</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.109161</td>\n",
       "      <td>0.967923</td>\n",
       "      <td>1.097724</td>\n",
       "      <td>2.307115</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.549763</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.788606</td>\n",
       "      <td>0.581332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-05</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>102.7</td>\n",
       "      <td>13603.93</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.06622</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.264163</td>\n",
       "      <td>0.718678</td>\n",
       "      <td>1.094089</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.710449</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.204864</td>\n",
       "      <td>0.824310</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.901202</td>\n",
       "      <td>0.836536</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.294707</td>\n",
       "      <td>1.168471</td>\n",
       "      <td>0.642743</td>\n",
       "      <td>0.878133</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.913503</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.149537</td>\n",
       "      <td>0.835657</td>\n",
       "      <td>2.780952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-06</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>102.7</td>\n",
       "      <td>13603.93</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.06622</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966656</td>\n",
       "      <td>1.606885</td>\n",
       "      <td>1.514631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.685084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833842</td>\n",
       "      <td>1.282477</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947752</td>\n",
       "      <td>2.059981</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980556</td>\n",
       "      <td>1.047620</td>\n",
       "      <td>2.572150</td>\n",
       "      <td>0.504080</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.596882</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.117321</td>\n",
       "      <td>1.553101</td>\n",
       "      <td>1.545230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-27</th>\n",
       "      <td>1.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>21525.82</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.66611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.543428</td>\n",
       "      <td>6.908116</td>\n",
       "      <td>218.0</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>128.639710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.900501</td>\n",
       "      <td>1.010875</td>\n",
       "      <td>1.764097</td>\n",
       "      <td>0.820641</td>\n",
       "      <td>0.808665</td>\n",
       "      <td>1.410060</td>\n",
       "      <td>1.415943</td>\n",
       "      <td>0.954658</td>\n",
       "      <td>0.776571</td>\n",
       "      <td>2.092578</td>\n",
       "      <td>0.829932</td>\n",
       "      <td>1.022473</td>\n",
       "      <td>1.347411</td>\n",
       "      <td>1.486079</td>\n",
       "      <td>0.898487</td>\n",
       "      <td>0.954025</td>\n",
       "      <td>1.223596</td>\n",
       "      <td>-7.016632</td>\n",
       "      <td>1.443049</td>\n",
       "      <td>0.882592</td>\n",
       "      <td>0.724875</td>\n",
       "      <td>0.591650</td>\n",
       "      <td>1.185079</td>\n",
       "      <td>0.728242</td>\n",
       "      <td>8.957344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-28</th>\n",
       "      <td>1.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>21525.82</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.66611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.556001</td>\n",
       "      <td>6.928135</td>\n",
       "      <td>218.0</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>128.219228</td>\n",
       "      <td>...</td>\n",
       "      <td>1.213115</td>\n",
       "      <td>1.395378</td>\n",
       "      <td>1.417193</td>\n",
       "      <td>0.968297</td>\n",
       "      <td>1.516745</td>\n",
       "      <td>1.109701</td>\n",
       "      <td>1.114263</td>\n",
       "      <td>1.105058</td>\n",
       "      <td>0.581300</td>\n",
       "      <td>1.697246</td>\n",
       "      <td>3.008876</td>\n",
       "      <td>0.982582</td>\n",
       "      <td>1.005550</td>\n",
       "      <td>0.918295</td>\n",
       "      <td>0.709275</td>\n",
       "      <td>0.988696</td>\n",
       "      <td>0.869484</td>\n",
       "      <td>3.268112</td>\n",
       "      <td>0.911087</td>\n",
       "      <td>0.984864</td>\n",
       "      <td>0.598464</td>\n",
       "      <td>0.436636</td>\n",
       "      <td>1.103782</td>\n",
       "      <td>1.596400</td>\n",
       "      <td>-4.887075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-29</th>\n",
       "      <td>1.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>21525.82</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.66611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.569604</td>\n",
       "      <td>6.950086</td>\n",
       "      <td>218.0</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>127.968071</td>\n",
       "      <td>...</td>\n",
       "      <td>1.506408</td>\n",
       "      <td>1.445075</td>\n",
       "      <td>0.717119</td>\n",
       "      <td>1.250066</td>\n",
       "      <td>1.680148</td>\n",
       "      <td>1.011607</td>\n",
       "      <td>0.793725</td>\n",
       "      <td>1.167880</td>\n",
       "      <td>0.557896</td>\n",
       "      <td>0.832619</td>\n",
       "      <td>1.914615</td>\n",
       "      <td>0.789692</td>\n",
       "      <td>0.580119</td>\n",
       "      <td>0.417745</td>\n",
       "      <td>0.626504</td>\n",
       "      <td>1.023202</td>\n",
       "      <td>0.493114</td>\n",
       "      <td>1.174228</td>\n",
       "      <td>0.410159</td>\n",
       "      <td>1.089446</td>\n",
       "      <td>0.832789</td>\n",
       "      <td>1.231150</td>\n",
       "      <td>1.131356</td>\n",
       "      <td>1.864625</td>\n",
       "      <td>2.523049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-30</th>\n",
       "      <td>1.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>21525.82</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.66611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.584262</td>\n",
       "      <td>6.974023</td>\n",
       "      <td>218.0</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>127.892806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997037</td>\n",
       "      <td>0.871637</td>\n",
       "      <td>0.660472</td>\n",
       "      <td>0.599521</td>\n",
       "      <td>0.656067</td>\n",
       "      <td>2.089199</td>\n",
       "      <td>0.552940</td>\n",
       "      <td>0.991999</td>\n",
       "      <td>0.843633</td>\n",
       "      <td>0.559295</td>\n",
       "      <td>0.826001</td>\n",
       "      <td>1.056501</td>\n",
       "      <td>0.899376</td>\n",
       "      <td>0.506841</td>\n",
       "      <td>0.975763</td>\n",
       "      <td>0.925346</td>\n",
       "      <td>0.793448</td>\n",
       "      <td>1.165541</td>\n",
       "      <td>0.758679</td>\n",
       "      <td>1.083804</td>\n",
       "      <td>1.150234</td>\n",
       "      <td>0.694483</td>\n",
       "      <td>1.647300</td>\n",
       "      <td>0.877406</td>\n",
       "      <td>0.370442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>1.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>21525.82</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.66611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>218.0</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888650</td>\n",
       "      <td>1.142839</td>\n",
       "      <td>1.553683</td>\n",
       "      <td>0.854794</td>\n",
       "      <td>0.895231</td>\n",
       "      <td>1.489947</td>\n",
       "      <td>1.094865</td>\n",
       "      <td>0.881892</td>\n",
       "      <td>1.260734</td>\n",
       "      <td>2.250674</td>\n",
       "      <td>2.198532</td>\n",
       "      <td>1.186660</td>\n",
       "      <td>1.332134</td>\n",
       "      <td>1.373651</td>\n",
       "      <td>1.007552</td>\n",
       "      <td>1.120675</td>\n",
       "      <td>1.415651</td>\n",
       "      <td>1.150159</td>\n",
       "      <td>2.028561</td>\n",
       "      <td>1.141342</td>\n",
       "      <td>1.033135</td>\n",
       "      <td>1.118354</td>\n",
       "      <td>1.453485</td>\n",
       "      <td>1.324952</td>\n",
       "      <td>2.656823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5051 rows × 442 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            GDPCQOQIndex  GDPCYOYIndex  GDPCTOT%Index  GPDITOC%Index  GPGSTOC%Index  \\\n",
       "date                                                                                  \n",
       "2006-01-02           5.4           3.4            4.5            6.1            5.2   \n",
       "2006-01-03           5.4           3.4            4.5            6.1            5.2   \n",
       "2006-01-04           5.4           3.4            4.5            6.1            5.2   \n",
       "2006-01-05           5.4           3.4            4.5            6.1            5.2   \n",
       "2006-01-06           5.4           3.4            4.5            6.1            5.2   \n",
       "...                  ...           ...            ...            ...            ...   \n",
       "2019-10-27           1.9           2.0            2.9           -1.5            2.0   \n",
       "2019-10-28           1.9           2.0            2.9           -1.5            2.0   \n",
       "2019-10-29           1.9           2.0            2.9           -1.5            2.0   \n",
       "2019-10-30           1.9           2.0            2.9           -1.5            2.0   \n",
       "2019-10-31           1.9           2.0            2.9           -1.5            2.0   \n",
       "\n",
       "            RGCDCIPIIndex  GDPCUR$Index  GDPCURYIndex  GDPPIQQIndex  GDPCPCECIndex  CPIYOYIndex  \\\n",
       "date                                                                                              \n",
       "2006-01-02          102.7      13603.93           6.6           2.8            2.3          4.0   \n",
       "2006-01-03          102.7      13603.93           6.6           2.8            2.3          4.0   \n",
       "2006-01-04          102.7      13603.93           6.6           2.8            2.3          4.0   \n",
       "2006-01-05          102.7      13603.93           6.6           2.8            2.3          4.0   \n",
       "2006-01-06          102.7      13603.93           6.6           2.8            2.3          4.0   \n",
       "...                   ...           ...           ...           ...            ...          ...   \n",
       "2019-10-27           69.0      21525.82           3.7           1.7            2.2          1.7   \n",
       "2019-10-28           69.0      21525.82           3.7           1.7            2.2          1.7   \n",
       "2019-10-29           69.0      21525.82           3.7           1.7            2.2          1.7   \n",
       "2019-10-30           69.0      21525.82           3.7           1.7            2.2          1.7   \n",
       "2019-10-31           69.0      21525.82           3.7           1.7            2.2          1.7   \n",
       "\n",
       "            CPIXYOYIndex  PCEDEFYIndex  PCECYOYIndex  CPICHNGIndex  CPUPXCHGIndex  FDIDFDMOIndex  \\\n",
       "date                                                                                               \n",
       "2006-01-02           2.1           3.2       2.06622           0.6            0.2            0.1   \n",
       "2006-01-03           2.1           3.2       2.06622           0.6            0.2            0.1   \n",
       "2006-01-04           2.1           3.2       2.06622           0.6            0.2            0.1   \n",
       "2006-01-05           2.1           3.2       2.06622           0.6            0.2            0.1   \n",
       "2006-01-06           2.1           3.2       2.06622           0.6            0.2            0.1   \n",
       "...                  ...           ...           ...           ...            ...            ...   \n",
       "2019-10-27           2.4           1.3       1.66611           0.0            0.1           -0.3   \n",
       "2019-10-28           2.4           1.3       1.66611           0.0            0.1           -0.3   \n",
       "2019-10-29           2.4           1.3       1.66611           0.0            0.1           -0.3   \n",
       "2019-10-30           2.4           1.3       1.66611           0.0            0.1           -0.3   \n",
       "2019-10-31           2.4           1.3       1.66611           0.0            0.1           -0.3   \n",
       "\n",
       "            FDIDSGMOIndex  FDIUFDYOIndex  FDIUSGYOIndex  USURTOTIndex  USUDMAERIndex  INJCJCIndex  \\\n",
       "date                                                                                                \n",
       "2006-01-02            0.3            2.6            2.2      4.700000       8.400000        326.0   \n",
       "2006-01-03            0.3            2.6            2.2      4.700000       8.400000        326.0   \n",
       "2006-01-04            0.3            2.6            2.2      4.700000       8.400000        326.0   \n",
       "2006-01-05            0.3            2.6            2.2      4.700000       8.400000        326.0   \n",
       "2006-01-06            0.3            2.6            2.2      4.700000       8.400000        326.0   \n",
       "...                   ...            ...            ...           ...            ...          ...   \n",
       "2019-10-27           -0.3            1.4            2.0      3.543428       6.908116        218.0   \n",
       "2019-10-28           -0.3            1.4            2.0      3.556001       6.928135        218.0   \n",
       "2019-10-29           -0.3            1.4            2.0      3.569604       6.950086        218.0   \n",
       "2019-10-30           -0.3            1.4            2.0      3.584262       6.974023        218.0   \n",
       "2019-10-31           -0.3            1.4            2.0      3.600000       7.000000        218.0   \n",
       "\n",
       "            INJCSPIndex  NFPTCHIndex  ...  AMD_product  APPLE_product  AppliedMaterials_product  \\\n",
       "date                                  ...                                                         \n",
       "2006-01-02       2561.0   278.000000  ...          NaN            NaN                       NaN   \n",
       "2006-01-03       2561.0   278.000000  ...     1.000000       1.000000                  1.000000   \n",
       "2006-01-04       2561.0   278.000000  ...     0.682030       0.771197                  1.026287   \n",
       "2006-01-05       2561.0   278.000000  ...     1.264163       0.718678                  1.094089   \n",
       "2006-01-06       2561.0   278.000000  ...     0.966656       1.606885                  1.514631   \n",
       "...                 ...          ...  ...          ...            ...                       ...   \n",
       "2019-10-27       1690.0   128.639710  ...     0.900501       1.010875                  1.764097   \n",
       "2019-10-28       1690.0   128.219228  ...     1.213115       1.395378                  1.417193   \n",
       "2019-10-29       1690.0   127.968071  ...     1.506408       1.445075                  0.717119   \n",
       "2019-10-30       1690.0   127.892806  ...     0.997037       0.871637                  0.660472   \n",
       "2019-10-31       1690.0   128.000000  ...     0.888650       1.142839                  1.553683   \n",
       "\n",
       "            Aspeed_product  Dell_product  Facebook_product  Google_product  HPE_product  \\\n",
       "date                                                                                      \n",
       "2006-01-02             NaN           NaN               NaN             NaN          NaN   \n",
       "2006-01-03        1.000000      1.000000          1.000000        1.000000     1.000000   \n",
       "2006-01-04        1.000000      1.000000          1.000000        1.223552     1.000000   \n",
       "2006-01-05        1.000000      1.000000          1.000000        0.710449     1.000000   \n",
       "2006-01-06        1.000000      1.000000          1.000000        1.685084     1.000000   \n",
       "...                    ...           ...               ...             ...          ...   \n",
       "2019-10-27        0.820641      0.808665          1.410060        1.415943     0.954658   \n",
       "2019-10-28        0.968297      1.516745          1.109701        1.114263     1.105058   \n",
       "2019-10-29        1.250066      1.680148          1.011607        0.793725     1.167880   \n",
       "2019-10-30        0.599521      0.656067          2.089199        0.552940     0.991999   \n",
       "2019-10-31        0.854794      0.895231          1.489947        1.094865     0.881892   \n",
       "\n",
       "            Intel_product  LamResearch_product  MediaTek_product  Micron_product  \\\n",
       "date                                                                               \n",
       "2006-01-02            NaN                  NaN               NaN             NaN   \n",
       "2006-01-03       1.000000             1.000000          1.000000        1.000000   \n",
       "2006-01-04       0.829714             0.967572          1.000000        1.380558   \n",
       "2006-01-05       1.204864             0.824310          1.000000        0.901202   \n",
       "2006-01-06       0.833842             1.282477          1.000000        0.947752   \n",
       "...                   ...                  ...               ...             ...   \n",
       "2019-10-27       0.776571             2.092578          0.829932        1.022473   \n",
       "2019-10-28       0.581300             1.697246          3.008876        0.982582   \n",
       "2019-10-29       0.557896             0.832619          1.914615        0.789692   \n",
       "2019-10-30       0.843633             0.559295          0.826001        1.056501   \n",
       "2019-10-31       1.260734             2.250674          2.198532        1.186660   \n",
       "\n",
       "            Microsoft_product  Nuvoton_product  Nvidia_product  Philadelphia_product  \\\n",
       "date                                                                                   \n",
       "2006-01-02                NaN              NaN             NaN                   NaN   \n",
       "2006-01-03           1.000000         1.000000        1.000000              1.000000   \n",
       "2006-01-04           0.743634         1.000000        1.109161              0.967923   \n",
       "2006-01-05           0.836536         1.000000        1.294707              1.168471   \n",
       "2006-01-06           2.059981         1.000000        0.980556              1.047620   \n",
       "...                       ...              ...             ...                   ...   \n",
       "2019-10-27           1.347411         1.486079        0.898487              0.954025   \n",
       "2019-10-28           1.005550         0.918295        0.709275              0.988696   \n",
       "2019-10-29           0.580119         0.417745        0.626504              1.023202   \n",
       "2019-10-30           0.899376         0.506841        0.975763              0.925346   \n",
       "2019-10-31           1.332134         1.373651        1.007552              1.120675   \n",
       "\n",
       "            QCOM_product  SamsungElectronics_product  ShinEtsuChemical_product  Siltronic_product  \\\n",
       "date                                                                                                \n",
       "2006-01-02           NaN                         NaN                       NaN                NaN   \n",
       "2006-01-03      1.000000                    0.862009                  1.000000           1.000000   \n",
       "2006-01-04      1.097724                    2.307115                  1.000000           1.000000   \n",
       "2006-01-05      0.642743                    0.878133                  1.000000           1.000000   \n",
       "2006-01-06      2.572150                    0.504080                  1.000000           1.000000   \n",
       "...                  ...                         ...                       ...                ...   \n",
       "2019-10-27      1.223596                   -7.016632                  1.443049           0.882592   \n",
       "2019-10-28      0.869484                    3.268112                  0.911087           0.984864   \n",
       "2019-10-29      0.493114                    1.174228                  0.410159           1.089446   \n",
       "2019-10-30      0.793448                    1.165541                  0.758679           1.083804   \n",
       "2019-10-31      1.415651                    1.150159                  2.028561           1.141342   \n",
       "\n",
       "            SKhynix_product  Sumco_product  TokyoElectron_product  TSMC_product  UMC_product  \n",
       "date                                                                                          \n",
       "2006-01-02              NaN            NaN                    NaN           NaN          NaN  \n",
       "2006-01-03         1.048766       1.000000               1.000000      1.000000     1.000000  \n",
       "2006-01-04         1.549763       1.000000               1.000000      0.788606     0.581332  \n",
       "2006-01-05         0.913503       1.000000               4.149537      0.835657     2.780952  \n",
       "2006-01-06         0.596882       1.000000               1.117321      1.553101     1.545230  \n",
       "...                     ...            ...                    ...           ...          ...  \n",
       "2019-10-27         0.724875       0.591650               1.185079      0.728242     8.957344  \n",
       "2019-10-28         0.598464       0.436636               1.103782      1.596400    -4.887075  \n",
       "2019-10-29         0.832789       1.231150               1.131356      1.864625     2.523049  \n",
       "2019-10-30         1.150234       0.694483               1.647300      0.877406     0.370442  \n",
       "2019-10-31         1.033135       1.118354               1.453485      1.324952     2.656823  \n",
       "\n",
       "[5051 rows x 442 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execution function\n",
    "# 새로운 데이터프레임 생성 \n",
    "cla.execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na for shifted data\n",
    "\n",
    "newdf = cla.df.fillna(method = 'ffill').fillna(method = 'bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check na\n",
    "\n",
    "newdf.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features\n",
    "\n",
    "len(newdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.drop('SamsungElectronics_product', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.to_csv('googleautomltest_120_200226.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>GDPCQOQIndex</th>\n",
       "      <th>GDPCYOYIndex</th>\n",
       "      <th>GDPCTOT%Index</th>\n",
       "      <th>GPDITOC%Index</th>\n",
       "      <th>GPGSTOC%Index</th>\n",
       "      <th>RGCDCIPIIndex</th>\n",
       "      <th>GDPCUR$Index</th>\n",
       "      <th>GDPCURYIndex</th>\n",
       "      <th>GDPPIQQIndex</th>\n",
       "      <th>GDPCPCECIndex</th>\n",
       "      <th>CPIYOYIndex</th>\n",
       "      <th>CPIXYOYIndex</th>\n",
       "      <th>PCEDEFYIndex</th>\n",
       "      <th>PCECYOYIndex</th>\n",
       "      <th>CPICHNGIndex</th>\n",
       "      <th>CPUPXCHGIndex</th>\n",
       "      <th>FDIDFDMOIndex</th>\n",
       "      <th>FDIDSGMOIndex</th>\n",
       "      <th>FDIUFDYOIndex</th>\n",
       "      <th>FDIUSGYOIndex</th>\n",
       "      <th>USURTOTIndex</th>\n",
       "      <th>USUDMAERIndex</th>\n",
       "      <th>INJCJCIndex</th>\n",
       "      <th>INJCSPIndex</th>\n",
       "      <th>...</th>\n",
       "      <th>AMD_product</th>\n",
       "      <th>APPLE_product</th>\n",
       "      <th>AppliedMaterials_product</th>\n",
       "      <th>Aspeed_product</th>\n",
       "      <th>Dell_product</th>\n",
       "      <th>Facebook_product</th>\n",
       "      <th>Google_product</th>\n",
       "      <th>HPE_product</th>\n",
       "      <th>Intel_product</th>\n",
       "      <th>LamResearch_product</th>\n",
       "      <th>MediaTek_product</th>\n",
       "      <th>Micron_product</th>\n",
       "      <th>Microsoft_product</th>\n",
       "      <th>Nuvoton_product</th>\n",
       "      <th>Nvidia_product</th>\n",
       "      <th>Philadelphia_product</th>\n",
       "      <th>QCOM_product</th>\n",
       "      <th>SamsungElectronics_product</th>\n",
       "      <th>ShinEtsuChemical_product</th>\n",
       "      <th>Siltronic_product</th>\n",
       "      <th>SKhynix_product</th>\n",
       "      <th>Sumco_product</th>\n",
       "      <th>TokyoElectron_product</th>\n",
       "      <th>TSMC_product</th>\n",
       "      <th>UMC_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-01-02</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>102.7</td>\n",
       "      <td>13603.93</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.06622</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.4</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.048766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-01-03</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>102.7</td>\n",
       "      <td>13603.93</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.06622</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.4</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.048766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-01-04</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>102.7</td>\n",
       "      <td>13603.93</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.06622</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.4</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.682030</td>\n",
       "      <td>0.771197</td>\n",
       "      <td>1.026287</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.223552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829714</td>\n",
       "      <td>0.967572</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.380558</td>\n",
       "      <td>0.743634</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.109161</td>\n",
       "      <td>0.967923</td>\n",
       "      <td>1.097724</td>\n",
       "      <td>2.307115</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.549763</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.788606</td>\n",
       "      <td>0.581332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-01-05</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>102.7</td>\n",
       "      <td>13603.93</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.06622</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.4</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.264163</td>\n",
       "      <td>0.718678</td>\n",
       "      <td>1.094089</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.710449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.204864</td>\n",
       "      <td>0.824310</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.901202</td>\n",
       "      <td>0.836536</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.294707</td>\n",
       "      <td>1.168471</td>\n",
       "      <td>0.642743</td>\n",
       "      <td>0.878133</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.913503</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.149537</td>\n",
       "      <td>0.835657</td>\n",
       "      <td>2.780952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-01-06</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>102.7</td>\n",
       "      <td>13603.93</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.06622</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.4</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966656</td>\n",
       "      <td>1.606885</td>\n",
       "      <td>1.514631</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.685084</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833842</td>\n",
       "      <td>1.282477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.947752</td>\n",
       "      <td>2.059981</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980556</td>\n",
       "      <td>1.047620</td>\n",
       "      <td>2.572150</td>\n",
       "      <td>0.504080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.596882</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.117321</td>\n",
       "      <td>1.553101</td>\n",
       "      <td>1.545230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 443 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  GDPCQOQIndex  GDPCYOYIndex  GDPCTOT%Index  GPDITOC%Index  GPGSTOC%Index  \\\n",
       "0 2006-01-02           5.4           3.4            4.5            6.1            5.2   \n",
       "1 2006-01-03           5.4           3.4            4.5            6.1            5.2   \n",
       "2 2006-01-04           5.4           3.4            4.5            6.1            5.2   \n",
       "3 2006-01-05           5.4           3.4            4.5            6.1            5.2   \n",
       "4 2006-01-06           5.4           3.4            4.5            6.1            5.2   \n",
       "\n",
       "   RGCDCIPIIndex  GDPCUR$Index  GDPCURYIndex  GDPPIQQIndex  GDPCPCECIndex  CPIYOYIndex  \\\n",
       "0          102.7      13603.93           6.6           2.8            2.3          4.0   \n",
       "1          102.7      13603.93           6.6           2.8            2.3          4.0   \n",
       "2          102.7      13603.93           6.6           2.8            2.3          4.0   \n",
       "3          102.7      13603.93           6.6           2.8            2.3          4.0   \n",
       "4          102.7      13603.93           6.6           2.8            2.3          4.0   \n",
       "\n",
       "   CPIXYOYIndex  PCEDEFYIndex  PCECYOYIndex  CPICHNGIndex  CPUPXCHGIndex  FDIDFDMOIndex  \\\n",
       "0           2.1           3.2       2.06622           0.6            0.2            0.1   \n",
       "1           2.1           3.2       2.06622           0.6            0.2            0.1   \n",
       "2           2.1           3.2       2.06622           0.6            0.2            0.1   \n",
       "3           2.1           3.2       2.06622           0.6            0.2            0.1   \n",
       "4           2.1           3.2       2.06622           0.6            0.2            0.1   \n",
       "\n",
       "   FDIDSGMOIndex  FDIUFDYOIndex  FDIUSGYOIndex  USURTOTIndex  USUDMAERIndex  INJCJCIndex  \\\n",
       "0            0.3            2.6            2.2           4.7            8.4        326.0   \n",
       "1            0.3            2.6            2.2           4.7            8.4        326.0   \n",
       "2            0.3            2.6            2.2           4.7            8.4        326.0   \n",
       "3            0.3            2.6            2.2           4.7            8.4        326.0   \n",
       "4            0.3            2.6            2.2           4.7            8.4        326.0   \n",
       "\n",
       "   INJCSPIndex  ...  AMD_product  APPLE_product  AppliedMaterials_product  Aspeed_product  \\\n",
       "0       2561.0  ...     1.000000       1.000000                  1.000000             1.0   \n",
       "1       2561.0  ...     1.000000       1.000000                  1.000000             1.0   \n",
       "2       2561.0  ...     0.682030       0.771197                  1.026287             1.0   \n",
       "3       2561.0  ...     1.264163       0.718678                  1.094089             1.0   \n",
       "4       2561.0  ...     0.966656       1.606885                  1.514631             1.0   \n",
       "\n",
       "   Dell_product  Facebook_product  Google_product  HPE_product  Intel_product  \\\n",
       "0           1.0               1.0        1.000000          1.0       1.000000   \n",
       "1           1.0               1.0        1.000000          1.0       1.000000   \n",
       "2           1.0               1.0        1.223552          1.0       0.829714   \n",
       "3           1.0               1.0        0.710449          1.0       1.204864   \n",
       "4           1.0               1.0        1.685084          1.0       0.833842   \n",
       "\n",
       "   LamResearch_product  MediaTek_product  Micron_product  Microsoft_product  Nuvoton_product  \\\n",
       "0             1.000000               1.0        1.000000           1.000000              1.0   \n",
       "1             1.000000               1.0        1.000000           1.000000              1.0   \n",
       "2             0.967572               1.0        1.380558           0.743634              1.0   \n",
       "3             0.824310               1.0        0.901202           0.836536              1.0   \n",
       "4             1.282477               1.0        0.947752           2.059981              1.0   \n",
       "\n",
       "   Nvidia_product  Philadelphia_product  QCOM_product  SamsungElectronics_product  \\\n",
       "0        1.000000              1.000000      1.000000                    0.862009   \n",
       "1        1.000000              1.000000      1.000000                    0.862009   \n",
       "2        1.109161              0.967923      1.097724                    2.307115   \n",
       "3        1.294707              1.168471      0.642743                    0.878133   \n",
       "4        0.980556              1.047620      2.572150                    0.504080   \n",
       "\n",
       "   ShinEtsuChemical_product  Siltronic_product  SKhynix_product  Sumco_product  \\\n",
       "0                       1.0                1.0         1.048766            1.0   \n",
       "1                       1.0                1.0         1.048766            1.0   \n",
       "2                       1.0                1.0         1.549763            1.0   \n",
       "3                       1.0                1.0         0.913503            1.0   \n",
       "4                       1.0                1.0         0.596882            1.0   \n",
       "\n",
       "   TokyoElectron_product  TSMC_product  UMC_product  \n",
       "0               1.000000      1.000000     1.000000  \n",
       "1               1.000000      1.000000     1.000000  \n",
       "2               1.000000      0.788606     0.581332  \n",
       "3               4.149537      0.835657     2.780952  \n",
       "4               1.117321      1.553101     1.545230  \n",
       "\n",
       "[5 rows x 443 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = newdf.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.drop('date', axis = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract target\n",
    "y_target = newdf['KRXsemiconductor_change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target drop\n",
    "newdf.drop(['KRXsemiconductor_change'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삼성전자가 이상하게 안됨 점검 필요 자꾸 infinity가 뜨므로 일단 제거\n",
    "newdf.drop('SamsungElectronics_product', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['LEIAVGWIndex5', 'KOIMPTIIndex120', 'KOHHLIndex1', 'KOEXPTIIndex20', 'KOEXTOTIndex260', 'SKLIMORDIndex260', 'KOHHDIndex1', 'KOBPFINIndex60', 'INJCSPIndex5', 'SKLINBARIndex5', 'KODIBALIndex260', 'INJCJCIndex20', 'FRNTTOTLIndex120', 'KORSTIndex260', 'EMPRGBCIIndex5', 'SKLILIIndex60', 'FRNTTNETIndex260', 'USWHTOTIndex1', 'SKLIMORDIndex120', 'SKBSICIndex60', 'MPMIUSSAIndex120', 'CONCCONFIndex60', 'COMFCOMFIndex120', 'MTSLRL$Index1', 'OEKRN022Index5', 'MPMIJPMAIndex20', 'NAPMNMIIndex1', 'KWCDCCurncy1', 'KOBSNMCIndex5', 'KOFDITIndex5', 'KOULMGFIndex120', 'SKCITTLIndex20', 'KOWDRIndex5', 'KOBPFINIndex1', 'KOHHDIndex20', 'LEIMNOIndex60', 'CONCCONFIndex20', 'SKLISVCIIndex60', 'SKLIWNRSIndex1', 'KOIMTOTIndex60', 'KOWDRIndex120', 'KOWDRIndex60', 'MTSLRL$Index5', 'SKLISVCIIndex120', 'KOBPFINIndex260', 'SKLICONEIndex120', 'OUTFGAFIndex20', 'NAPMNEWOIndex5', 'KOIPMCIndex5', 'KOBPCAIndex260', 'NAPMPMIIndex5', 'FRNTTOTLIndex1', 'PIDSPINXIndex120', 'EMDINP1MIndex1', 'KOBSNMCIndex20', 'AWHTOTLIndex60', 'USTBTOTIndex20', 'KOEXTOTIndex20', 'LEIAVGWIndex1', 'KOTRBALIndex5', 'LEINWCNIndex5', 'FRNTTNETIndex60', 'IPIndex120', 'KOBSNMCIndex120', 'SKLIMORDIndex60', 'MPMIUSMAIndex5', 'OEKRN022Index1', 'USWHTOTIndex120', 'USTBIMPIndex5', 'EMPRGBCIIndex20', 'KOFDITIndex1', 'KOGFBALIndex1', 'MPMIJPMAIndex5', 'KOBONTLIndex5', 'MAPMINDXIndex1', 'EMPRGBCIIndex120', 'SKLILIIndex1', 'LEIWKIJIndex60'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-50c09de96327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnewdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'KOSPI200_change'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dell_volume'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOIMPTIIndex120'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'USWHTOTIndex120'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MTSLRL$Index1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LEINWCNIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SKLIMORDIndex260'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOIPMCIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CONCCONFIndex60'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OUTFGAFIndex20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOHHDIndex20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'QCOM_change'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OEKRN022Index1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOIMTOTIndex60'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOBPFINIndex260'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EMPRGBCIIndex120'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MPMIJPMAIndex20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ShinEtsuChemical_change'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'COMFCOMFIndex120'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ShinEtsuChemical_close'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KODIBALIndex260'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MPMIUSMAIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOECSEMQIndex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOBPFINIndex60'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MPMIUSSAIndex120'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FRNTTOTLIndex120'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOFDITIndex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'INJCJCIndex20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOWDRIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MPMIJPMAIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'USDollarIndex_change'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MTSLRL$Index5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KWCDCCurncy1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NAPMNEWOIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOPSIYIndex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Nvidia_product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOBPFINIndex1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Intel_product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SKLIMORDIndex120'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Facebook_product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KODSDISCIndex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LamResearch_change'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UMC_change'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LEIMNOIndex60'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOBSNMCIndex120'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KORSTIndex260'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EMDINP1MIndex1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AppliedMaterials_volume'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOHSTRIndex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Philadelphia_product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TSMC_volume'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOHHLIndex1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'USWHTOTIndex1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOWDRIndex120'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LEIAVGWIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'USTBIMPIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOWDRIndex60'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FRNTTNETIndex60'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Micron_product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TSMC_change'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TokyoElectron_change'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Micron_volume'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOTRBALIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ShinEtsuChemical_volume'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SKLISVCIIndex60'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SKLIWNRSIndex1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NAPMPMIIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'APPLE_product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOBPCAIndex260'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SKLISVCIIndex120'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOIPOPSMIndex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MediaTek_change'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOBPCAIndex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOBONTLIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AMD_product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOEXTOTIndex20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SKhynix_change'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NAPMNMIIndex1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SKLILIIndex1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOFDITIndex1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FRNTTNETIndex260'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MAPMINDXIndex1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOFDITIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AppliedMaterials_product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOEXTOTIndex260'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOSPI200_volume'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'IPIndex120'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOHHDIndex1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Microsoft_product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOMSM2YIndex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOCGCGSMIndex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SKhynix_product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UMC_product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'USTBTOTIndex20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOGFBALIndex1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OEKRN022Index5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOHCTTLIndex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LEIWKIJIndex60'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EMPRGBCIIndex20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SKLICONEIndex120'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Amazon_product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FRNTTOTLIndex1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SKLINBARIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EUR_KRW_volume'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'JPY_KRW_volume'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'USDollarIndex_volume'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'USD_KRW_volume'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PIDSPINXIndex120'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EOKOS002Index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOEXPTIIndex20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOEXPTIYIndex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LEIAVGWIndex1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOULMGFIndex120'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PITLCHNGIndex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOBSNMCIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KOBSNMCIndex20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ShinEtsuChemical_product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Google_product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TokyoElectron_product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GVSK10YRIndex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SKBSICIndex60'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UMC_volume'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GDPPIQQIndex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'INJCSPIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'EMPRGBCIIndex5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SKLILIIndex60'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SKCITTLIndex20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Aspeed_close'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SKLIMORDIndex60'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AWHTOTLIndex60'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CONCCONFIndex20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Amazon_volume'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TokyoElectron_volume'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-py37/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2999\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3001\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-py37/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1283\u001b[0m                 \u001b[0;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"raise_missing\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-py37/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m         )\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf2-py37/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"loc\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} not in index\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnot_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['LEIAVGWIndex5', 'KOIMPTIIndex120', 'KOHHLIndex1', 'KOEXPTIIndex20', 'KOEXTOTIndex260', 'SKLIMORDIndex260', 'KOHHDIndex1', 'KOBPFINIndex60', 'INJCSPIndex5', 'SKLINBARIndex5', 'KODIBALIndex260', 'INJCJCIndex20', 'FRNTTOTLIndex120', 'KORSTIndex260', 'EMPRGBCIIndex5', 'SKLILIIndex60', 'FRNTTNETIndex260', 'USWHTOTIndex1', 'SKLIMORDIndex120', 'SKBSICIndex60', 'MPMIUSSAIndex120', 'CONCCONFIndex60', 'COMFCOMFIndex120', 'MTSLRL$Index1', 'OEKRN022Index5', 'MPMIJPMAIndex20', 'NAPMNMIIndex1', 'KWCDCCurncy1', 'KOBSNMCIndex5', 'KOFDITIndex5', 'KOULMGFIndex120', 'SKCITTLIndex20', 'KOWDRIndex5', 'KOBPFINIndex1', 'KOHHDIndex20', 'LEIMNOIndex60', 'CONCCONFIndex20', 'SKLISVCIIndex60', 'SKLIWNRSIndex1', 'KOIMTOTIndex60', 'KOWDRIndex120', 'KOWDRIndex60', 'MTSLRL$Index5', 'SKLISVCIIndex120', 'KOBPFINIndex260', 'SKLICONEIndex120', 'OUTFGAFIndex20', 'NAPMNEWOIndex5', 'KOIPMCIndex5', 'KOBPCAIndex260', 'NAPMPMIIndex5', 'FRNTTOTLIndex1', 'PIDSPINXIndex120', 'EMDINP1MIndex1', 'KOBSNMCIndex20', 'AWHTOTLIndex60', 'USTBTOTIndex20', 'KOEXTOTIndex20', 'LEIAVGWIndex1', 'KOTRBALIndex5', 'LEINWCNIndex5', 'FRNTTNETIndex60', 'IPIndex120', 'KOBSNMCIndex120', 'SKLIMORDIndex60', 'MPMIUSMAIndex5', 'OEKRN022Index1', 'USWHTOTIndex120', 'USTBIMPIndex5', 'EMPRGBCIIndex20', 'KOFDITIndex1', 'KOGFBALIndex1', 'MPMIJPMAIndex5', 'KOBONTLIndex5', 'MAPMINDXIndex1', 'EMPRGBCIIndex120', 'SKLILIIndex1', 'LEIWKIJIndex60'] not in index\""
     ]
    }
   ],
   "source": [
    "# 이 지점에서 각자 잡은 feature를 넣어주자\n",
    "\n",
    "\n",
    "newdf = newdf[['KOSPI200_change', 'Dell_volume', 'KOIMPTIIndex120', 'USWHTOTIndex120', 'MTSLRL$Index1', 'LEINWCNIndex5', 'SKLIMORDIndex260', 'KOIPMCIndex5', 'CONCCONFIndex60', 'OUTFGAFIndex20', 'KOHHDIndex20', 'QCOM_change', 'OEKRN022Index1', 'KOIMTOTIndex60', 'KOBPFINIndex260', 'EMPRGBCIIndex120', 'MPMIJPMAIndex20', 'ShinEtsuChemical_change', 'COMFCOMFIndex120', 'ShinEtsuChemical_close', 'KODIBALIndex260', 'MPMIUSMAIndex5', 'KOECSEMQIndex', 'KOBPFINIndex60', 'MPMIUSSAIndex120', 'FRNTTOTLIndex120', 'KOFDITIndex', 'INJCJCIndex20', 'KOWDRIndex5', 'MPMIJPMAIndex5', 'USDollarIndex_change', 'MTSLRL$Index5', 'KWCDCCurncy1', 'NAPMNEWOIndex5', 'KOPSIYIndex', 'Nvidia_product', 'KOBPFINIndex1', 'Intel_product', 'SKLIMORDIndex120', 'Facebook_product', 'KODSDISCIndex', 'LamResearch_change', 'UMC_change', 'LEIMNOIndex60', 'KOBSNMCIndex120', 'KORSTIndex260', 'EMDINP1MIndex1', 'AppliedMaterials_volume', 'KOHSTRIndex', 'Philadelphia_product', 'TSMC_volume', 'KOHHLIndex1', 'USWHTOTIndex1', 'KOWDRIndex120', 'LEIAVGWIndex5', 'USTBIMPIndex5', 'KOWDRIndex60', 'FRNTTNETIndex60', 'Micron_product', 'TSMC_change', 'TokyoElectron_change', 'g2', 'Micron_volume', 'KOTRBALIndex5', 'ShinEtsuChemical_volume', 'SKLISVCIIndex60', 'SKLIWNRSIndex1', 'NAPMPMIIndex5', 'APPLE_product', 'KOBPCAIndex260', 'SKLISVCIIndex120', 'KOIPOPSMIndex', 'MediaTek_change', 'KOBPCAIndex', 'KOBONTLIndex5', 'AMD_product', 'KOEXTOTIndex20', 'SKhynix_change', 'NAPMNMIIndex1', 'SKLILIIndex1', 'KOFDITIndex1', 'FRNTTNETIndex260', 'MAPMINDXIndex1', 'KOFDITIndex5', 'AppliedMaterials_product', 'KOEXTOTIndex260', 'KOSPI200_volume', 'IPIndex120', 'KOHHDIndex1', 'Microsoft_product', 'KOMSM2YIndex', 'KOCGCGSMIndex', 'SKhynix_product', 'UMC_product', 'USTBTOTIndex20', 'KOGFBALIndex1', 'OEKRN022Index5', 'KOHCTTLIndex', 'LEIWKIJIndex60', 'EMPRGBCIIndex20', 'SKLICONEIndex120', 'Amazon_product', 'FRNTTOTLIndex1', 'SKLINBARIndex5', 'EUR_KRW_volume', 'JPY_KRW_volume', 'USDollarIndex_volume', 'USD_KRW_volume', 'PIDSPINXIndex120', 'EOKOS002Index', 'KOEXPTIIndex20', 'KOEXPTIYIndex', 'LEIAVGWIndex1', 'KOULMGFIndex120', 'PITLCHNGIndex', 'KOBSNMCIndex5', 'KOBSNMCIndex20', 'ShinEtsuChemical_product', 'Google_product', 'TokyoElectron_product', 'GVSK10YRIndex', 'SKBSICIndex60', 'UMC_volume', 'GDPPIQQIndex', 'INJCSPIndex5', 'EMPRGBCIIndex5', 'SKLILIIndex60', 'SKCITTLIndex20', 'Aspeed_close', 'SKLIMORDIndex60', 'AWHTOTLIndex60', 'CONCCONFIndex20', 'Amazon_volume', 'TokyoElectron_volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDPCQOQIndex</th>\n",
       "      <th>GDPCYOYIndex</th>\n",
       "      <th>GDPCTOT%Index</th>\n",
       "      <th>GPDITOC%Index</th>\n",
       "      <th>GPGSTOC%Index</th>\n",
       "      <th>RGCDCIPIIndex</th>\n",
       "      <th>GDPCUR$Index</th>\n",
       "      <th>GDPCURYIndex</th>\n",
       "      <th>GDPPIQQIndex</th>\n",
       "      <th>GDPCPCECIndex</th>\n",
       "      <th>CPIYOYIndex</th>\n",
       "      <th>CPIXYOYIndex</th>\n",
       "      <th>PCEDEFYIndex</th>\n",
       "      <th>PCECYOYIndex</th>\n",
       "      <th>CPICHNGIndex</th>\n",
       "      <th>CPUPXCHGIndex</th>\n",
       "      <th>FDIDFDMOIndex</th>\n",
       "      <th>FDIDSGMOIndex</th>\n",
       "      <th>FDIUFDYOIndex</th>\n",
       "      <th>FDIUSGYOIndex</th>\n",
       "      <th>USURTOTIndex</th>\n",
       "      <th>USUDMAERIndex</th>\n",
       "      <th>INJCJCIndex</th>\n",
       "      <th>INJCSPIndex</th>\n",
       "      <th>NFPTCHIndex</th>\n",
       "      <th>...</th>\n",
       "      <th>Amazon_product</th>\n",
       "      <th>AMD_product</th>\n",
       "      <th>APPLE_product</th>\n",
       "      <th>AppliedMaterials_product</th>\n",
       "      <th>Aspeed_product</th>\n",
       "      <th>Dell_product</th>\n",
       "      <th>Facebook_product</th>\n",
       "      <th>Google_product</th>\n",
       "      <th>HPE_product</th>\n",
       "      <th>Intel_product</th>\n",
       "      <th>LamResearch_product</th>\n",
       "      <th>MediaTek_product</th>\n",
       "      <th>Micron_product</th>\n",
       "      <th>Microsoft_product</th>\n",
       "      <th>Nuvoton_product</th>\n",
       "      <th>Nvidia_product</th>\n",
       "      <th>Philadelphia_product</th>\n",
       "      <th>QCOM_product</th>\n",
       "      <th>ShinEtsuChemical_product</th>\n",
       "      <th>Siltronic_product</th>\n",
       "      <th>SKhynix_product</th>\n",
       "      <th>Sumco_product</th>\n",
       "      <th>TokyoElectron_product</th>\n",
       "      <th>TSMC_product</th>\n",
       "      <th>UMC_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>102.7</td>\n",
       "      <td>13603.93</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.06622</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.4</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.048766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>102.7</td>\n",
       "      <td>13603.93</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.06622</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.4</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.048766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>102.7</td>\n",
       "      <td>13603.93</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.06622</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.4</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990868</td>\n",
       "      <td>0.682030</td>\n",
       "      <td>0.771197</td>\n",
       "      <td>1.026287</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.223552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829714</td>\n",
       "      <td>0.967572</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.380558</td>\n",
       "      <td>0.743634</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.109161</td>\n",
       "      <td>0.967923</td>\n",
       "      <td>1.097724</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.549763</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.788606</td>\n",
       "      <td>0.581332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>102.7</td>\n",
       "      <td>13603.93</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.06622</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.4</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735048</td>\n",
       "      <td>1.264163</td>\n",
       "      <td>0.718678</td>\n",
       "      <td>1.094089</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.710449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.204864</td>\n",
       "      <td>0.824310</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.901202</td>\n",
       "      <td>0.836536</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.294707</td>\n",
       "      <td>1.168471</td>\n",
       "      <td>0.642743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.913503</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.149537</td>\n",
       "      <td>0.835657</td>\n",
       "      <td>2.780952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>102.7</td>\n",
       "      <td>13603.93</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.06622</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.4</td>\n",
       "      <td>326.0</td>\n",
       "      <td>2561.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.139744</td>\n",
       "      <td>0.966656</td>\n",
       "      <td>1.606885</td>\n",
       "      <td>1.514631</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.685084</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833842</td>\n",
       "      <td>1.282477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.947752</td>\n",
       "      <td>2.059981</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980556</td>\n",
       "      <td>1.047620</td>\n",
       "      <td>2.572150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.596882</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.117321</td>\n",
       "      <td>1.553101</td>\n",
       "      <td>1.545230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 440 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GDPCQOQIndex  GDPCYOYIndex  GDPCTOT%Index  GPDITOC%Index  GPGSTOC%Index  RGCDCIPIIndex  \\\n",
       "0           5.4           3.4            4.5            6.1            5.2          102.7   \n",
       "1           5.4           3.4            4.5            6.1            5.2          102.7   \n",
       "2           5.4           3.4            4.5            6.1            5.2          102.7   \n",
       "3           5.4           3.4            4.5            6.1            5.2          102.7   \n",
       "4           5.4           3.4            4.5            6.1            5.2          102.7   \n",
       "\n",
       "   GDPCUR$Index  GDPCURYIndex  GDPPIQQIndex  GDPCPCECIndex  CPIYOYIndex  CPIXYOYIndex  \\\n",
       "0      13603.93           6.6           2.8            2.3          4.0           2.1   \n",
       "1      13603.93           6.6           2.8            2.3          4.0           2.1   \n",
       "2      13603.93           6.6           2.8            2.3          4.0           2.1   \n",
       "3      13603.93           6.6           2.8            2.3          4.0           2.1   \n",
       "4      13603.93           6.6           2.8            2.3          4.0           2.1   \n",
       "\n",
       "   PCEDEFYIndex  PCECYOYIndex  CPICHNGIndex  CPUPXCHGIndex  FDIDFDMOIndex  FDIDSGMOIndex  \\\n",
       "0           3.2       2.06622           0.6            0.2            0.1            0.3   \n",
       "1           3.2       2.06622           0.6            0.2            0.1            0.3   \n",
       "2           3.2       2.06622           0.6            0.2            0.1            0.3   \n",
       "3           3.2       2.06622           0.6            0.2            0.1            0.3   \n",
       "4           3.2       2.06622           0.6            0.2            0.1            0.3   \n",
       "\n",
       "   FDIUFDYOIndex  FDIUSGYOIndex  USURTOTIndex  USUDMAERIndex  INJCJCIndex  INJCSPIndex  \\\n",
       "0            2.6            2.2           4.7            8.4        326.0       2561.0   \n",
       "1            2.6            2.2           4.7            8.4        326.0       2561.0   \n",
       "2            2.6            2.2           4.7            8.4        326.0       2561.0   \n",
       "3            2.6            2.2           4.7            8.4        326.0       2561.0   \n",
       "4            2.6            2.2           4.7            8.4        326.0       2561.0   \n",
       "\n",
       "   NFPTCHIndex  ...  Amazon_product  AMD_product  APPLE_product  AppliedMaterials_product  \\\n",
       "0        278.0  ...        1.000000     1.000000       1.000000                  1.000000   \n",
       "1        278.0  ...        1.000000     1.000000       1.000000                  1.000000   \n",
       "2        278.0  ...        0.990868     0.682030       0.771197                  1.026287   \n",
       "3        278.0  ...        0.735048     1.264163       0.718678                  1.094089   \n",
       "4        278.0  ...        1.139744     0.966656       1.606885                  1.514631   \n",
       "\n",
       "   Aspeed_product  Dell_product  Facebook_product  Google_product  HPE_product  Intel_product  \\\n",
       "0             1.0           1.0               1.0        1.000000          1.0       1.000000   \n",
       "1             1.0           1.0               1.0        1.000000          1.0       1.000000   \n",
       "2             1.0           1.0               1.0        1.223552          1.0       0.829714   \n",
       "3             1.0           1.0               1.0        0.710449          1.0       1.204864   \n",
       "4             1.0           1.0               1.0        1.685084          1.0       0.833842   \n",
       "\n",
       "   LamResearch_product  MediaTek_product  Micron_product  Microsoft_product  Nuvoton_product  \\\n",
       "0             1.000000               1.0        1.000000           1.000000              1.0   \n",
       "1             1.000000               1.0        1.000000           1.000000              1.0   \n",
       "2             0.967572               1.0        1.380558           0.743634              1.0   \n",
       "3             0.824310               1.0        0.901202           0.836536              1.0   \n",
       "4             1.282477               1.0        0.947752           2.059981              1.0   \n",
       "\n",
       "   Nvidia_product  Philadelphia_product  QCOM_product  ShinEtsuChemical_product  \\\n",
       "0        1.000000              1.000000      1.000000                       1.0   \n",
       "1        1.000000              1.000000      1.000000                       1.0   \n",
       "2        1.109161              0.967923      1.097724                       1.0   \n",
       "3        1.294707              1.168471      0.642743                       1.0   \n",
       "4        0.980556              1.047620      2.572150                       1.0   \n",
       "\n",
       "   Siltronic_product  SKhynix_product  Sumco_product  TokyoElectron_product  TSMC_product  \\\n",
       "0                1.0         1.048766            1.0               1.000000      1.000000   \n",
       "1                1.0         1.048766            1.0               1.000000      1.000000   \n",
       "2                1.0         1.549763            1.0               1.000000      0.788606   \n",
       "3                1.0         0.913503            1.0               4.149537      0.835657   \n",
       "4                1.0         0.596882            1.0               1.117321      1.553101   \n",
       "\n",
       "   UMC_product  \n",
       "0     1.000000  \n",
       "1     1.000000  \n",
       "2     0.581332  \n",
       "3     2.780952  \n",
       "4     1.545230  \n",
       "\n",
       "[5 rows x 440 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling 하기전에 columns 뽑기 \n",
    "col_name = newdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling과 동시에 변환\n",
    "#newdf = PowerTransformer().fit_transform(newdf)\n",
    "newdf = StandardScaler().fit_transform(newdf)\n",
    "#newdf = newdf.apply(lambda x : np.log1p(x))\n",
    "#newdf = MinMaxScaler().fit_transfrom(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling하면 colname사라지므로 다시 생성\n",
    "newdf = pd.DataFrame(newdf, columns = col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract X_data\n",
    "X_data = newdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random split과 not random split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size = 0.3, random_state = 156)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size = 0.2, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Feature Selection\n",
    "## 안돌려도 무방"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-138-dcc4a0f6e932>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-138-dcc4a0f6e932>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    feature_selected_final = feature_selected[:(np.array(score).argmax() + 1)]\u001b[0m\n\u001b[0m                                                                              \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "## Feature Selection Code\n",
    "# Forward stepwise feature selection\n",
    "# linear model 기준으로 유효 feature 뽑기. 돌리지 않아도 되는 코드\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "feature_name = list(X_train.columns)\n",
    "feature_selected = []\n",
    "score = []\n",
    "\n",
    "for i in range(X_train.shape[1]):\n",
    "    score_temp_list = np.zeros(len(feature_name))\n",
    "    for j in range(len(feature_name)):\n",
    "        temp = feature_selected.copy() \n",
    "        temp.append(feature_name[j])\n",
    "        x_temp = X_train[temp]\n",
    "        lr_temp = LinearRegression() \n",
    "        lr_temp.fit(x_temp, y_train)\n",
    "        predict_temp = lr_temp.predict(X_test[temp])\n",
    "        r2_temp = r2_score(y_test, predict_temp)\n",
    "        score_temp = r2_temp\n",
    "        score_temp_list[j] = score_temp \n",
    "        \n",
    "    temp = feature_name[score_temp_list.argmax()]\n",
    "    feature_selected.append(temp)\n",
    "    feature_name.remove(temp)\n",
    "    score.append(score_temp_list.max())\n",
    "    print(\"%02d Selected: \"%i, feature_selected)\n",
    "    print(\"%02d Score : \"%i, np.round(10000*np.array(score))/10000)\n",
    "        \n",
    "# finally selected features\n",
    "feature_selected_final = feature_selected[:(np.array(score).argmax() + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric 구성\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score\n",
    "\n",
    "class CustomMetric():\n",
    "\n",
    "    def __init__(self, pred, real):\n",
    "            self.pred = pred\n",
    "            self.real = real\n",
    "            self.thr25 = np.percentile(self.real, 25)\n",
    "            self.thr75 = np.percentile(self.real, 75)\n",
    "            self.df = pd.DataFrame({'pred' : self.pred, 'real' : self.real})\n",
    "        \n",
    "    def issame(self, data1, data2):\n",
    "        if data1 > 0 and data2 > 0 : \n",
    "            return 'T'\n",
    "        elif data1 > 0 and data2 < 0 :\n",
    "            return 'F'\n",
    "        elif data1 < 0 and data2 > 0 :\n",
    "            return 'F'\n",
    "        elif data1 < 0 and data2 < 0 :\n",
    "            return 'T'\n",
    "        elif data1 == 0 or data2 == 0 :\n",
    "            return 0\n",
    "        else :\n",
    "            return 'notcal'\n",
    "\n",
    "    def getouter(self, data1, data2): #quantile_25 = np.percentile(values, 25)\n",
    "\n",
    "        if data1 > 0 and data2 >= self.thr75 :\n",
    "            return 'T'\n",
    "        elif data1 < 0 and data2 <= self.thr25 :\n",
    "            return 'T'\n",
    "        elif data1 >= 0 and data2 <= self.thr25 :\n",
    "            return 'F'\n",
    "        elif data1 <= 0 and data2 >= self.thr75: \n",
    "            return 'F'\n",
    "        else:\n",
    "            return 'notcal'\n",
    "        \n",
    "    def makedf(self):\n",
    "        self.df['TF'] = self.df.apply(lambda x : self.issame(x['pred'], x['real']), axis = 1)\n",
    "        self.df['thrTF'] = self.df.apply(lambda x : self.getouter(x['pred'], x['real']), axis = 1)\n",
    "        \n",
    "        return self.df\n",
    "        \n",
    "    def execution(self):\n",
    "        mdf = pd.DataFrame()\n",
    "        mdf['CORR'] = [self.df['real'].corr(self.df['pred'], method = 'pearson')]\n",
    "        mdf['R2'] = [\"{0:0.4f}\".format(r2_score(self.df['real'], self.df['pred']))]\n",
    "        mdf['MAE'] = [\"{0:0.4f}\".format(mean_absolute_error(self.df['pred'], self.df['real']))]\n",
    "        mdf['RMSE'] = [\"{0:0.4f}\".format(np.sqrt(mean_squared_error(self.df['pred'], self.df['real'])))]\n",
    "        mdf['ACR'] = [sum(self.df['TF'] == 'T')/len(self.df['TF'])]\n",
    "        \n",
    "        mdf['threshACR'] = [sum(self.df['thrTF'] == 'T') / sum(self.df['thrTF'] != 'notcal')]\n",
    "        \n",
    "        return mdf     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear model select\n",
    "lr_reg = LinearRegression()\n",
    "#ridge = Ridge(alpha = 100)\n",
    "#lasso = Lasso(alpha = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reg.fit(X_train, y_train)\n",
    "y_pred = lr_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_reg = DecisionTreeRegressor(random_state = 0, max_depth = 100)\n",
    "rf_reg = RandomForestRegressor(random_state = 0, n_estimators = 1000)\n",
    "gb_reg = GradientBoostingRegressor(random_state = 0, n_estimators = 1000)\n",
    "xgb_reg = XGBRegressor(n_estimators = 1000)\n",
    "lgb_reg = LGBMRegressor(n_estimators = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric function 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CORR</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>ACR</th>\n",
       "      <th>threshACR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.315524</td>\n",
       "      <td>0.0987</td>\n",
       "      <td>1.4147</td>\n",
       "      <td>1.8702</td>\n",
       "      <td>0.570722</td>\n",
       "      <td>0.609467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CORR      R2     MAE    RMSE       ACR  threshACR\n",
       "0  0.315524  0.0987  1.4147  1.8702  0.570722   0.609467"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metr = CustomMetric(y_pred, y_test)\n",
    "metr.makedf()\n",
    "metr.execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Model은 회귀계수로 나오므로 아래와 같이 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_imp = pd.DataFrame(sorted(zip(lr_reg.coef_, X_train.columns)), columns = ['Value', 'Feature'])\n",
    "coef_imp.sort_values(by = 'Value', ascending = False)\n",
    "plt.figure(figsize = (10, 30))\n",
    "sns.barplot(x = 'Value', y = 'Feature', data = coef_imp.sort_values(by = 'Value', ascending = False))\n",
    "plt.title('Linear Regressor Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree Model은 feature importance가 따로 뽑히므로 아래와 같이 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.DataFrame(sorted(zip(lgb_reg.feature_importances_, X_train.columns)), columns = ['Value', 'Feature'])\n",
    "feat_imp.sort_values(by = 'Value', ascending = False)\n",
    "plt.figure(figsize = (10, 30))\n",
    "sns.barplot(x = 'Value', y = 'Feature', data = feat_imp.sort_values(by = 'Value', ascending = False))\n",
    "plt.title('LightGBM Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances_1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 번외 : Optuna AutoML을 통한 LightGBM Hyperparameter tuning 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "import sklearn.model_selection\n",
    "from sklearn.metrics import r2_score\n",
    "import optuna\n",
    "import optuna.integration.lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_feature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_feature_fraction, val_score: 3.743039:  14%|#4        | 1/7 [00:00<00:03,  1.96it/s]\u001b[32m[I 2020-02-11 18:02:45,383]\u001b[0m Finished trial#0 resulted in value: 3.743039295223581. Current best value is 3.743039295223581 with parameters: {'feature_fraction': 0.4}.\u001b[0m\n",
      "tune_feature_fraction, val_score: 3.743039:  14%|#4        | 1/7 [00:00<00:03,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 1.14427\tvalid_1's l2: 4.37173\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.35093\tvalid_1's l2: 3.74304\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_feature_fraction, val_score: 3.743039:  29%|##8       | 2/7 [00:01<00:02,  1.98it/s]\u001b[32m[I 2020-02-11 18:02:45,881]\u001b[0m Finished trial#1 resulted in value: 3.7694221469647315. Current best value is 3.743039295223581 with parameters: {'feature_fraction': 0.4}.\u001b[0m\n",
      "tune_feature_fraction, val_score: 3.743039:  29%|##8       | 2/7 [00:01<00:02,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 1.13166\tvalid_1's l2: 4.4361\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's l2: 4.45106\tvalid_1's l2: 3.76942\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_feature_fraction, val_score: 3.743039:  43%|####2     | 3/7 [00:01<00:02,  1.84it/s]\u001b[32m[I 2020-02-11 18:02:46,511]\u001b[0m Finished trial#2 resulted in value: 3.7865627785977276. Current best value is 3.743039295223581 with parameters: {'feature_fraction': 0.4}.\u001b[0m\n",
      "tune_feature_fraction, val_score: 3.743039:  43%|####2     | 3/7 [00:01<00:02,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 1.10724\tvalid_1's l2: 4.3306\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's l2: 4.45355\tvalid_1's l2: 3.78656\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_feature_fraction, val_score: 3.743039:  57%|#####7    | 4/7 [00:02<00:01,  1.73it/s]\u001b[32m[I 2020-02-11 18:02:47,164]\u001b[0m Finished trial#3 resulted in value: 3.7565527076227063. Current best value is 3.743039295223581 with parameters: {'feature_fraction': 0.4}.\u001b[0m\n",
      "tune_feature_fraction, val_score: 3.743039:  57%|#####7    | 4/7 [00:02<00:01,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 1.06745\tvalid_1's l2: 4.42483\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.34275\tvalid_1's l2: 3.75655\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_feature_fraction, val_score: 3.743039:  71%|#######1  | 5/7 [00:02<00:01,  1.73it/s]\u001b[32m[I 2020-02-11 18:02:47,748]\u001b[0m Finished trial#4 resulted in value: 3.7675234068430754. Current best value is 3.743039295223581 with parameters: {'feature_fraction': 0.4}.\u001b[0m\n",
      "tune_feature_fraction, val_score: 3.743039:  71%|#######1  | 5/7 [00:02<00:01,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 1.04923\tvalid_1's l2: 4.19544\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.33672\tvalid_1's l2: 3.76752\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_feature_fraction, val_score: 3.743039:  86%|########5 | 6/7 [00:03<00:00,  1.53it/s]\u001b[32m[I 2020-02-11 18:02:48,572]\u001b[0m Finished trial#5 resulted in value: 3.848364833873613. Current best value is 3.743039295223581 with parameters: {'feature_fraction': 0.4}.\u001b[0m\n",
      "tune_feature_fraction, val_score: 3.743039:  86%|########5 | 6/7 [00:03<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 1.05643\tvalid_1's l2: 4.68182\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's l2: 4.45704\tvalid_1's l2: 3.84836\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_feature_fraction, val_score: 3.743039: 100%|##########| 7/7 [00:04<00:00,  1.61it/s]\u001b[32m[I 2020-02-11 18:02:49,118]\u001b[0m Finished trial#6 resulted in value: 3.8606053544201333. Current best value is 3.743039295223581 with parameters: {'feature_fraction': 0.4}.\u001b[0m\n",
      "tune_feature_fraction, val_score: 3.743039: 100%|##########| 7/7 [00:04<00:00,  1.63it/s]\n",
      "tune_num_leaves, val_score: 3.743039:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.989447\tvalid_1's l2: 4.47646\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's l2: 4.45269\tvalid_1's l2: 3.86061\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.735221:   5%|5         | 1/20 [00:01<00:23,  1.21s/it]\u001b[32m[I 2020-02-11 18:02:50,387]\u001b[0m Finished trial#0 resulted in value: 3.735220592924571. Current best value is 3.735220592924571 with parameters: {'num_leaves': 214}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.735221:   5%|5         | 1/20 [00:01<00:23,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.107172\tvalid_1's l2: 4.48901\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.1154\tvalid_1's l2: 3.73522\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.735221:  10%|#         | 2/20 [00:02<00:21,  1.17s/it]\u001b[32m[I 2020-02-11 18:02:51,472]\u001b[0m Finished trial#1 resulted in value: 3.735220592924571. Current best value is 3.735220592924571 with parameters: {'num_leaves': 214}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.735221:  10%|#         | 2/20 [00:02<00:21,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.107172\tvalid_1's l2: 4.48901\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.1154\tvalid_1's l2: 3.73522\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.735221:  15%|#5        | 3/20 [00:03<00:20,  1.20s/it]\u001b[32m[I 2020-02-11 18:02:52,743]\u001b[0m Finished trial#2 resulted in value: 3.7504955549707115. Current best value is 3.735220592924571 with parameters: {'num_leaves': 214}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.735221:  15%|#5        | 3/20 [00:03<00:20,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.23057\tvalid_1's l2: 4.37326\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.19343\tvalid_1's l2: 3.7505\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.735221:  20%|##        | 4/20 [00:04<00:18,  1.17s/it]\u001b[32m[I 2020-02-11 18:02:53,827]\u001b[0m Finished trial#3 resulted in value: 3.735220592924571. Current best value is 3.735220592924571 with parameters: {'num_leaves': 214}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.735221:  20%|##        | 4/20 [00:04<00:18,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.107172\tvalid_1's l2: 4.48901\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.1154\tvalid_1's l2: 3.73522\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.735221:  25%|##5       | 5/20 [00:05<00:16,  1.07s/it]\u001b[32m[I 2020-02-11 18:02:54,688]\u001b[0m Finished trial#4 resulted in value: 3.756219050882487. Current best value is 3.735220592924571 with parameters: {'num_leaves': 214}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.735221:  25%|##5       | 5/20 [00:05<00:16,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.197672\tvalid_1's l2: 4.16885\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.18319\tvalid_1's l2: 3.75622\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.735221:  30%|###       | 6/20 [00:06<00:15,  1.10s/it]\u001b[32m[I 2020-02-11 18:02:55,854]\u001b[0m Finished trial#5 resulted in value: 3.768136207598261. Current best value is 3.735220592924571 with parameters: {'num_leaves': 214}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.735221:  30%|###       | 6/20 [00:06<00:15,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.163086\tvalid_1's l2: 4.47864\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.16611\tvalid_1's l2: 3.76814\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.735221:  35%|###5      | 7/20 [00:08<00:18,  1.39s/it]\u001b[32m[I 2020-02-11 18:02:57,898]\u001b[0m Finished trial#6 resulted in value: 3.735220592924571. Current best value is 3.735220592924571 with parameters: {'num_leaves': 214}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.735221:  35%|###5      | 7/20 [00:08<00:18,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.107172\tvalid_1's l2: 4.48901\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.1154\tvalid_1's l2: 3.73522\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.735221:  40%|####      | 8/20 [00:10<00:18,  1.55s/it]\u001b[32m[I 2020-02-11 18:02:59,837]\u001b[0m Finished trial#7 resulted in value: 3.735220592924571. Current best value is 3.735220592924571 with parameters: {'num_leaves': 214}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.735221:  40%|####      | 8/20 [00:10<00:18,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.107172\tvalid_1's l2: 4.48901\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.1154\tvalid_1's l2: 3.73522\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.735221:  45%|####5     | 9/20 [00:11<00:15,  1.45s/it]\u001b[32m[I 2020-02-11 18:03:01,046]\u001b[0m Finished trial#8 resulted in value: 3.740980363405558. Current best value is 3.735220592924571 with parameters: {'num_leaves': 214}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.735221:  45%|####5     | 9/20 [00:11<00:15,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.108789\tvalid_1's l2: 4.5437\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.1219\tvalid_1's l2: 3.74098\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.735221:  50%|#####     | 10/20 [00:13<00:13,  1.38s/it]\u001b[32m[I 2020-02-11 18:03:02,288]\u001b[0m Finished trial#9 resulted in value: 3.7685868161301714. Current best value is 3.735220592924571 with parameters: {'num_leaves': 214}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.735221:  50%|#####     | 10/20 [00:13<00:13,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.15994\tvalid_1's l2: 4.60279\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.16203\tvalid_1's l2: 3.76859\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.731570:  55%|#####5    | 11/20 [00:13<00:09,  1.07s/it]\u001b[32m[I 2020-02-11 18:03:02,613]\u001b[0m Finished trial#10 resulted in value: 3.731569542412721. Current best value is 3.731569542412721 with parameters: {'num_leaves': 15}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.731570:  60%|######    | 12/20 [00:13<00:06,  1.27it/s]\u001b[32m[I 2020-02-11 18:03:02,736]\u001b[0m Finished trial#11 resulted in value: 3.7518110998846352. Current best value is 3.731569542412721 with parameters: {'num_leaves': 15}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.731570:  60%|######    | 12/20 [00:13<00:06,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.19025\tvalid_1's l2: 4.31079\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's l2: 4.50694\tvalid_1's l2: 3.73157\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's l2: 4.32042\tvalid_1's l2: 3.88483\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.57488\tvalid_1's l2: 3.75181\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.731570:  65%|######5   | 13/20 [00:13<00:04,  1.68it/s]\u001b[32m[I 2020-02-11 18:03:02,883]\u001b[0m Finished trial#12 resulted in value: 3.7494616911379484. Current best value is 3.731569542412721 with parameters: {'num_leaves': 15}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.731570:  65%|######5   | 13/20 [00:13<00:04,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 3.7473\tvalid_1's l2: 4.32062\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.53344\tvalid_1's l2: 3.74946\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.731570:  70%|#######   | 14/20 [00:15<00:04,  1.22it/s]\u001b[32m[I 2020-02-11 18:03:04,218]\u001b[0m Finished trial#13 resulted in value: 3.735220592924571. Current best value is 3.731569542412721 with parameters: {'num_leaves': 15}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.731570:  70%|#######   | 14/20 [00:15<00:04,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.107172\tvalid_1's l2: 4.48901\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.1154\tvalid_1's l2: 3.73522\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.731570:  75%|#######5  | 15/20 [00:15<00:03,  1.30it/s]\u001b[32m[I 2020-02-11 18:03:04,872]\u001b[0m Finished trial#14 resulted in value: 3.7389696293551546. Current best value is 3.731569542412721 with parameters: {'num_leaves': 15}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.731570:  75%|#######5  | 15/20 [00:15<00:03,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.753086\tvalid_1's l2: 4.57753\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.29803\tvalid_1's l2: 3.73897\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.731570:  80%|########  | 16/20 [00:17<00:04,  1.04s/it]\u001b[32m[I 2020-02-11 18:03:06,564]\u001b[0m Finished trial#15 resulted in value: 3.735220592924571. Current best value is 3.731569542412721 with parameters: {'num_leaves': 15}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.731570:  80%|########  | 16/20 [00:17<00:04,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.107172\tvalid_1's l2: 4.48901\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.1154\tvalid_1's l2: 3.73522\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.731570:  85%|########5 | 17/20 [00:17<00:02,  1.13it/s]\u001b[32m[I 2020-02-11 18:03:07,062]\u001b[0m Finished trial#16 resulted in value: 3.739077006227947. Current best value is 3.731569542412721 with parameters: {'num_leaves': 15}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.731570:  85%|########5 | 17/20 [00:17<00:02,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.781549\tvalid_1's l2: 4.33439\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.3024\tvalid_1's l2: 3.73908\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.731570:  90%|######### | 18/20 [00:19<00:02,  1.04s/it]\u001b[32m[I 2020-02-11 18:03:08,489]\u001b[0m Finished trial#17 resulted in value: 3.739571691030669. Current best value is 3.731569542412721 with parameters: {'num_leaves': 15}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.731570:  90%|######### | 18/20 [00:19<00:02,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.112003\tvalid_1's l2: 4.57667\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.12728\tvalid_1's l2: 3.73957\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.731570:  95%|#########5| 19/20 [00:20<00:01,  1.21s/it]\u001b[32m[I 2020-02-11 18:03:10,086]\u001b[0m Finished trial#18 resulted in value: 3.735220592924571. Current best value is 3.731569542412721 with parameters: {'num_leaves': 15}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.731570:  95%|#########5| 19/20 [00:20<00:01,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.107172\tvalid_1's l2: 4.48901\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.1154\tvalid_1's l2: 3.73522\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_num_leaves, val_score: 3.731570: 100%|##########| 20/20 [00:21<00:00,  1.02s/it]\u001b[32m[I 2020-02-11 18:03:10,675]\u001b[0m Finished trial#19 resulted in value: 3.750199068394421. Current best value is 3.731569542412721 with parameters: {'num_leaves': 15}.\u001b[0m\n",
      "tune_num_leaves, val_score: 3.731570: 100%|##########| 20/20 [00:21<00:00,  1.08s/it]\n",
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.731570:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.642406\tvalid_1's l2: 4.41874\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.27882\tvalid_1's l2: 3.7502\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.704491:  10%|#         | 1/10 [00:00<00:01,  4.92it/s]\u001b[32m[I 2020-02-11 18:03:10,936]\u001b[0m Finished trial#0 resulted in value: 3.704490809895457. Current best value is 3.704490809895457 with parameters: {'bagging_fraction': 0.9846476358806338, 'bagging_freq': 5}.\u001b[0m\n",
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.704491:  10%|#         | 1/10 [00:00<00:01,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.17483\tvalid_1's l2: 4.11138\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.44053\tvalid_1's l2: 3.70449\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  20%|##        | 2/10 [00:00<00:01,  4.42it/s]\u001b[32m[I 2020-02-11 18:03:11,216]\u001b[0m Finished trial#1 resulted in value: 3.693085549207284. Current best value is 3.693085549207284 with parameters: {'bagging_fraction': 0.7225765831000759, 'bagging_freq': 1}.\u001b[0m\n",
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  20%|##        | 2/10 [00:00<00:01,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.20261\tvalid_1's l2: 4.42469\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43778\tvalid_1's l2: 3.69309\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  30%|###       | 3/10 [00:00<00:02,  3.45it/s]\u001b[32m[I 2020-02-11 18:03:11,655]\u001b[0m Finished trial#2 resulted in value: 3.7747270526961327. Current best value is 3.693085549207284 with parameters: {'bagging_fraction': 0.7225765831000759, 'bagging_freq': 1}.\u001b[0m\n",
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  30%|###       | 3/10 [00:00<00:02,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.24708\tvalid_1's l2: 4.36171\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's l2: 4.51632\tvalid_1's l2: 3.77473\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  40%|####      | 4/10 [00:01<00:01,  3.52it/s]\u001b[32m[I 2020-02-11 18:03:11,929]\u001b[0m Finished trial#3 resulted in value: 3.7391765177587026. Current best value is 3.693085549207284 with parameters: {'bagging_fraction': 0.7225765831000759, 'bagging_freq': 1}.\u001b[0m\n",
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  40%|####      | 4/10 [00:01<00:01,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.14561\tvalid_1's l2: 4.29109\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's l2: 4.51101\tvalid_1's l2: 3.73918\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  50%|#####     | 5/10 [00:01<00:01,  3.47it/s]\u001b[32m[I 2020-02-11 18:03:12,222]\u001b[0m Finished trial#4 resulted in value: 3.7687333055249015. Current best value is 3.693085549207284 with parameters: {'bagging_fraction': 0.7225765831000759, 'bagging_freq': 1}.\u001b[0m\n",
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  50%|#####     | 5/10 [00:01<00:01,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.15463\tvalid_1's l2: 4.58127\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43508\tvalid_1's l2: 3.76873\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  60%|######    | 6/10 [00:01<00:01,  3.70it/s]\u001b[32m[I 2020-02-11 18:03:12,457]\u001b[0m Finished trial#5 resulted in value: 3.767117485799114. Current best value is 3.693085549207284 with parameters: {'bagging_fraction': 0.7225765831000759, 'bagging_freq': 1}.\u001b[0m\n",
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  60%|######    | 6/10 [00:01<00:01,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.18257\tvalid_1's l2: 4.2411\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's l2: 4.50745\tvalid_1's l2: 3.76712\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  70%|#######   | 7/10 [00:02<00:00,  3.59it/s]\u001b[32m[I 2020-02-11 18:03:12,758]\u001b[0m Finished trial#6 resulted in value: 3.7255405576165215. Current best value is 3.693085549207284 with parameters: {'bagging_fraction': 0.7225765831000759, 'bagging_freq': 1}.\u001b[0m\n",
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  70%|#######   | 7/10 [00:02<00:00,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.15296\tvalid_1's l2: 4.18\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.42798\tvalid_1's l2: 3.72554\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  80%|########  | 8/10 [00:02<00:00,  3.59it/s]\u001b[32m[I 2020-02-11 18:03:13,029]\u001b[0m Finished trial#7 resulted in value: 3.7720268532453587. Current best value is 3.693085549207284 with parameters: {'bagging_fraction': 0.7225765831000759, 'bagging_freq': 1}.\u001b[0m\n",
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  80%|########  | 8/10 [00:02<00:00,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.1925\tvalid_1's l2: 4.42254\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.45791\tvalid_1's l2: 3.77203\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  90%|######### | 9/10 [00:02<00:00,  3.68it/s]\u001b[32m[I 2020-02-11 18:03:13,283]\u001b[0m Finished trial#8 resulted in value: 3.6971380704784944. Current best value is 3.693085549207284 with parameters: {'bagging_fraction': 0.7225765831000759, 'bagging_freq': 1}.\u001b[0m\n",
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086:  90%|######### | 9/10 [00:02<00:00,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.19251\tvalid_1's l2: 4.24742\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.44539\tvalid_1's l2: 3.69714\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086: 100%|##########| 10/10 [00:02<00:00,  3.59it/s]\u001b[32m[I 2020-02-11 18:03:13,578]\u001b[0m Finished trial#9 resulted in value: 3.7136995447104506. Current best value is 3.693085549207284 with parameters: {'bagging_fraction': 0.7225765831000759, 'bagging_freq': 1}.\u001b[0m\n",
      "tune_bagging_fraction_and_bagging_freq, val_score: 3.693086: 100%|##########| 10/10 [00:02<00:00,  3.45it/s]\n",
      "tune_feature_fraction, val_score: 3.693086:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.15005\tvalid_1's l2: 4.27549\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43851\tvalid_1's l2: 3.7137\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_feature_fraction, val_score: 3.693086:  33%|###3      | 1/3 [00:00<00:00,  4.72it/s]\u001b[32m[I 2020-02-11 18:03:13,848]\u001b[0m Finished trial#0 resulted in value: 3.693085549207284. Current best value is 3.693085549207284 with parameters: {'feature_fraction': 0.41600000000000004}.\u001b[0m\n",
      "tune_feature_fraction, val_score: 3.693086:  33%|###3      | 1/3 [00:00<00:00,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.16136\tvalid_1's l2: 4.55744\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43778\tvalid_1's l2: 3.69309\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_feature_fraction, val_score: 3.693086:  67%|######6   | 2/3 [00:00<00:00,  3.61it/s]\u001b[32m[I 2020-02-11 18:03:14,274]\u001b[0m Finished trial#1 resulted in value: 3.75545761318674. Current best value is 3.693085549207284 with parameters: {'feature_fraction': 0.41600000000000004}.\u001b[0m\n",
      "tune_feature_fraction, val_score: 3.693086:  67%|######6   | 2/3 [00:00<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.12625\tvalid_1's l2: 4.4212\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's l2: 4.50849\tvalid_1's l2: 3.75546\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_feature_fraction, val_score: 3.693086: 100%|##########| 3/3 [00:00<00:00,  3.65it/s]\u001b[32m[I 2020-02-11 18:03:14,542]\u001b[0m Finished trial#2 resulted in value: 3.75545761318674. Current best value is 3.693085549207284 with parameters: {'feature_fraction': 0.41600000000000004}.\u001b[0m\n",
      "tune_feature_fraction, val_score: 3.693086: 100%|##########| 3/3 [00:00<00:00,  3.12it/s]\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.13224\tvalid_1's l2: 4.36267\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's l2: 4.50849\tvalid_1's l2: 3.75546\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:   5%|5         | 1/20 [00:00<00:04,  4.13it/s]\u001b[32m[I 2020-02-11 18:03:14,842]\u001b[0m Finished trial#0 resulted in value: 3.6931224435506356. Current best value is 3.6931224435506356 with parameters: {'lambda_l1': 0.0004091354760566695, 'lambda_l2': 0.015454082789431823}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:   5%|5         | 1/20 [00:00<00:04,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.20321\tvalid_1's l2: 4.42422\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43786\tvalid_1's l2: 3.69312\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  10%|#         | 2/20 [00:00<00:04,  4.11it/s]\u001b[32m[I 2020-02-11 18:03:15,086]\u001b[0m Finished trial#1 resulted in value: 3.69964872383405. Current best value is 3.6931224435506356 with parameters: {'lambda_l1': 0.0004091354760566695, 'lambda_l2': 0.015454082789431823}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  10%|#         | 2/20 [00:00<00:04,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.22339\tvalid_1's l2: 4.38409\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.4443\tvalid_1's l2: 3.69965\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  15%|#5        | 3/20 [00:00<00:04,  4.17it/s]\u001b[32m[I 2020-02-11 18:03:15,318]\u001b[0m Finished trial#2 resulted in value: 3.7723058642600455. Current best value is 3.6931224435506356 with parameters: {'lambda_l1': 0.0004091354760566695, 'lambda_l2': 0.015454082789431823}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  15%|#5        | 3/20 [00:00<00:04,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.37843\tvalid_1's l2: 4.30682\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.47339\tvalid_1's l2: 3.77231\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  20%|##        | 4/20 [00:00<00:03,  4.14it/s]\u001b[32m[I 2020-02-11 18:03:15,563]\u001b[0m Finished trial#3 resulted in value: 3.6930856496798548. Current best value is 3.6930856496798548 with parameters: {'lambda_l1': 1.2746027537109919e-08, 'lambda_l2': 4.267659803128042e-05}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  20%|##        | 4/20 [00:01<00:03,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.20261\tvalid_1's l2: 4.42469\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43778\tvalid_1's l2: 3.69309\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  25%|##5       | 5/20 [00:01<00:03,  3.77it/s]\u001b[32m[I 2020-02-11 18:03:15,883]\u001b[0m Finished trial#4 resulted in value: 3.69390744817483. Current best value is 3.6930856496798548 with parameters: {'lambda_l1': 1.2746027537109919e-08, 'lambda_l2': 4.267659803128042e-05}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  25%|##5       | 5/20 [00:01<00:03,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.17227\tvalid_1's l2: 4.32682\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43961\tvalid_1's l2: 3.69391\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  30%|###       | 6/20 [00:01<00:03,  3.70it/s]\u001b[32m[I 2020-02-11 18:03:16,175]\u001b[0m Finished trial#5 resulted in value: 3.696966717255604. Current best value is 3.6930856496798548 with parameters: {'lambda_l1': 1.2746027537109919e-08, 'lambda_l2': 4.267659803128042e-05}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  30%|###       | 6/20 [00:01<00:03,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.20741\tvalid_1's l2: 4.41957\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.44121\tvalid_1's l2: 3.69697\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  35%|###5      | 7/20 [00:01<00:03,  3.27it/s]\u001b[32m[I 2020-02-11 18:03:16,557]\u001b[0m Finished trial#6 resulted in value: 3.752889718813778. Current best value is 3.6930856496798548 with parameters: {'lambda_l1': 1.2746027537109919e-08, 'lambda_l2': 4.267659803128042e-05}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  35%|###5      | 7/20 [00:02<00:03,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.25461\tvalid_1's l2: 4.23827\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.4547\tvalid_1's l2: 3.75289\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  40%|####      | 8/20 [00:02<00:03,  3.41it/s]\u001b[32m[I 2020-02-11 18:03:16,819]\u001b[0m Finished trial#7 resulted in value: 3.6932798470758272. Current best value is 3.6930856496798548 with parameters: {'lambda_l1': 1.2746027537109919e-08, 'lambda_l2': 4.267659803128042e-05}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  40%|####      | 8/20 [00:02<00:03,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.16689\tvalid_1's l2: 4.47396\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43811\tvalid_1's l2: 3.69328\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  45%|####5     | 9/20 [00:02<00:03,  3.34it/s]\u001b[32m[I 2020-02-11 18:03:17,134]\u001b[0m Finished trial#8 resulted in value: 3.7017072062649756. Current best value is 3.6930856496798548 with parameters: {'lambda_l1': 1.2746027537109919e-08, 'lambda_l2': 4.267659803128042e-05}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  45%|####5     | 9/20 [00:02<00:03,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.21972\tvalid_1's l2: 4.15326\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.44716\tvalid_1's l2: 3.70171\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  50%|#####     | 10/20 [00:02<00:02,  3.47it/s]\u001b[32m[I 2020-02-11 18:03:17,394]\u001b[0m Finished trial#9 resulted in value: 3.693126416548805. Current best value is 3.6930856496798548 with parameters: {'lambda_l1': 1.2746027537109919e-08, 'lambda_l2': 4.267659803128042e-05}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  50%|#####     | 10/20 [00:02<00:02,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.20334\tvalid_1's l2: 4.42413\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43786\tvalid_1's l2: 3.69313\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  55%|#####5    | 11/20 [00:03<00:02,  3.59it/s]\u001b[32m[I 2020-02-11 18:03:17,653]\u001b[0m Finished trial#10 resulted in value: 3.693085549950353. Current best value is 3.693085549950353 with parameters: {'lambda_l1': 1.872072499566791e-08, 'lambda_l2': 3.0537428441761945e-07}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  55%|#####5    | 11/20 [00:03<00:02,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.20261\tvalid_1's l2: 4.42469\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43778\tvalid_1's l2: 3.69309\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  60%|######    | 12/20 [00:03<00:02,  3.30it/s]\u001b[32m[I 2020-02-11 18:03:18,012]\u001b[0m Finished trial#11 resulted in value: 3.6930855497933504. Current best value is 3.6930855497933504 with parameters: {'lambda_l1': 1.8281151832217688e-08, 'lambda_l2': 2.3890353551064293e-07}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  60%|######    | 12/20 [00:03<00:02,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.20261\tvalid_1's l2: 4.42469\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43778\tvalid_1's l2: 3.69309\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  65%|######5   | 13/20 [00:03<00:01,  3.52it/s]\u001b[32m[I 2020-02-11 18:03:18,251]\u001b[0m Finished trial#12 resulted in value: 3.693085549295667. Current best value is 3.693085549295667 with parameters: {'lambda_l1': 1.4674443979062974e-08, 'lambda_l2': 2.9426028891628908e-08}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  65%|######5   | 13/20 [00:03<00:01,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.20261\tvalid_1's l2: 4.42469\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43778\tvalid_1's l2: 3.69309\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  70%|#######   | 14/20 [00:03<00:01,  3.66it/s]\u001b[32m[I 2020-02-11 18:03:18,497]\u001b[0m Finished trial#13 resulted in value: 3.693085551280851. Current best value is 3.693085549295667 with parameters: {'lambda_l1': 1.4674443979062974e-08, 'lambda_l2': 2.9426028891628908e-08}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  70%|#######   | 14/20 [00:03<00:01,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.20261\tvalid_1's l2: 4.42469\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43778\tvalid_1's l2: 3.69309\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  75%|#######5  | 15/20 [00:04<00:01,  3.81it/s]\u001b[32m[I 2020-02-11 18:03:18,740]\u001b[0m Finished trial#14 resulted in value: 3.693085551473689. Current best value is 3.693085549295667 with parameters: {'lambda_l1': 1.4674443979062974e-08, 'lambda_l2': 2.9426028891628908e-08}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  75%|#######5  | 15/20 [00:04<00:01,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.20261\tvalid_1's l2: 4.42469\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43778\tvalid_1's l2: 3.69309\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  80%|########  | 16/20 [00:04<00:01,  3.68it/s]\u001b[32m[I 2020-02-11 18:03:19,029]\u001b[0m Finished trial#15 resulted in value: 3.693085549368055. Current best value is 3.693085549295667 with parameters: {'lambda_l1': 1.4674443979062974e-08, 'lambda_l2': 2.9426028891628908e-08}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  80%|########  | 16/20 [00:04<00:01,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.20261\tvalid_1's l2: 4.42469\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43778\tvalid_1's l2: 3.69309\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  85%|########5 | 17/20 [00:04<00:00,  3.86it/s]\u001b[32m[I 2020-02-11 18:03:19,261]\u001b[0m Finished trial#16 resulted in value: 3.693085564903099. Current best value is 3.693085549295667 with parameters: {'lambda_l1': 1.4674443979062974e-08, 'lambda_l2': 2.9426028891628908e-08}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  85%|########5 | 17/20 [00:04<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.20261\tvalid_1's l2: 4.42469\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43778\tvalid_1's l2: 3.69309\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  90%|######### | 18/20 [00:04<00:00,  3.86it/s]\u001b[32m[I 2020-02-11 18:03:19,523]\u001b[0m Finished trial#17 resulted in value: 3.6930855879777025. Current best value is 3.693085549295667 with parameters: {'lambda_l1': 1.4674443979062974e-08, 'lambda_l2': 2.9426028891628908e-08}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  90%|######### | 18/20 [00:04<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.20261\tvalid_1's l2: 4.42469\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43778\tvalid_1's l2: 3.69309\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  95%|#########5| 19/20 [00:05<00:00,  3.68it/s]\u001b[32m[I 2020-02-11 18:03:19,827]\u001b[0m Finished trial#18 resulted in value: 3.693085602211528. Current best value is 3.693085549295667 with parameters: {'lambda_l1': 1.4674443979062974e-08, 'lambda_l2': 2.9426028891628908e-08}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086:  95%|#########5| 19/20 [00:05<00:00,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.20261\tvalid_1's l2: 4.42469\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43778\tvalid_1's l2: 3.69309\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086: 100%|##########| 20/20 [00:05<00:00,  3.75it/s]\u001b[32m[I 2020-02-11 18:03:20,073]\u001b[0m Finished trial#19 resulted in value: 3.6930855494426478. Current best value is 3.693085549295667 with parameters: {'lambda_l1': 1.4674443979062974e-08, 'lambda_l2': 2.9426028891628908e-08}.\u001b[0m\n",
      "tune_lambda_l1_and_lambda_l2, val_score: 3.693086: 100%|##########| 20/20 [00:05<00:00,  3.62it/s]\n",
      "tune_min_child_samples, val_score: 3.693086:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.20261\tvalid_1's l2: 4.42469\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.43778\tvalid_1's l2: 3.69309\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_min_child_samples, val_score: 3.673303:  20%|##        | 1/5 [00:00<00:00,  4.66it/s]\u001b[32m[I 2020-02-11 18:03:20,343]\u001b[0m Finished trial#0 resulted in value: 3.6733028125293257. Current best value is 3.6733028125293257 with parameters: {'min_child_samples': 5}.\u001b[0m\n",
      "tune_min_child_samples, val_score: 3.673303:  20%|##        | 1/5 [00:00<00:00,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 1.96797\tvalid_1's l2: 4.58294\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.39761\tvalid_1's l2: 3.6733\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_min_child_samples, val_score: 3.673303:  40%|####      | 2/5 [00:00<00:00,  4.36it/s]\u001b[32m[I 2020-02-11 18:03:20,607]\u001b[0m Finished trial#1 resulted in value: 3.696103143346446. Current best value is 3.6733028125293257 with parameters: {'min_child_samples': 5}.\u001b[0m\n",
      "tune_min_child_samples, val_score: 3.673303:  40%|####      | 2/5 [00:00<00:00,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.03812\tvalid_1's l2: 4.34099\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.42925\tvalid_1's l2: 3.6961\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_min_child_samples, val_score: 3.673303:  60%|######    | 3/5 [00:00<00:00,  4.36it/s]\u001b[32m[I 2020-02-11 18:03:20,842]\u001b[0m Finished trial#2 resulted in value: 3.767396359996281. Current best value is 3.6733028125293257 with parameters: {'min_child_samples': 5}.\u001b[0m\n",
      "tune_min_child_samples, val_score: 3.673303:  60%|######    | 3/5 [00:00<00:00,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.23093\tvalid_1's l2: 4.53729\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.44043\tvalid_1's l2: 3.7674\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_min_child_samples, val_score: 3.673303:  80%|########  | 4/5 [00:01<00:00,  3.90it/s]\u001b[32m[I 2020-02-11 18:03:21,156]\u001b[0m Finished trial#3 resulted in value: 3.7783779775951953. Current best value is 3.6733028125293257 with parameters: {'min_child_samples': 5}.\u001b[0m\n",
      "tune_min_child_samples, val_score: 3.673303:  80%|########  | 4/5 [00:01<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 2.43349\tvalid_1's l2: 4.29193\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's l2: 4.52346\tvalid_1's l2: 3.77838\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's l2: 2.72726\tvalid_1's l2: 4.30335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tune_min_child_samples, val_score: 3.673303: 100%|##########| 5/5 [00:01<00:00,  4.15it/s]\u001b[32m[I 2020-02-11 18:03:21,367]\u001b[0m Finished trial#4 resulted in value: 3.7465455442848334. Current best value is 3.6733028125293257 with parameters: {'min_child_samples': 5}.\u001b[0m\n",
      "tune_min_child_samples, val_score: 3.673303: 100%|##########| 5/5 [00:01<00:00,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l2: 4.48874\tvalid_1's l2: 3.74655\n",
      "Number of finished trials: 65\n",
      "Best params: {'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 15, 'feature_fraction': 0.4, 'bagging_fraction': 0.7225765831000759, 'bagging_freq': 1, 'min_child_samples': 5}\n",
      "  r2_score = -0.0458964390586587\n",
      "  Params: \n",
      "    lambda_l1: 0.0\n",
      "    lambda_l2: 0.0\n",
      "    num_leaves: 15\n",
      "    feature_fraction: 0.4\n",
      "    bagging_fraction: 0.7225765831000759\n",
      "    bagging_freq: 1\n",
      "    min_child_samples: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dval = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'l2',\n",
    "    'verbosity': -1,\n",
    "    'boosting_type': 'gbdt',\n",
    "}\n",
    "\n",
    "best_params, tuning_history = dict(), list()\n",
    "\n",
    "model = lgb.train(params,\n",
    "                  dtrain,\n",
    "                  valid_sets=[dtrain, dval],\n",
    "                  best_params=best_params,\n",
    "                  tuning_history=tuning_history,\n",
    "                  verbose_eval=100,\n",
    "                  early_stopping_rounds=100,\n",
    "                  )\n",
    "\n",
    "prediction = np.rint(model.predict(X_test, num_iteration=model.best_iteration))\n",
    "r2_score = r2_score(y_test, prediction)\n",
    "\n",
    "print('Number of finished trials: {}'.format(len(tuning_history)))\n",
    "print('Best params:', best_params)\n",
    "print('  r2_score = {}'.format(r2_score))\n",
    "print('  Params: ')\n",
    "for key, value in best_params.items():\n",
    "    print('    {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning된 값 입력하여 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_reg = LGBMRegressor(lambda_l1 = 0.6885870703417802, lambda_l2 = 1.1741445421730667, num_leaves = 33, feature_fraction = 0.8999999999999999, bagging_fraction = 1.0, bagging_freq = 0, min_child_samples = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.4999, bagging_freq=6, boosting_type='gbdt',\n",
       "              class_weight=None, colsample_bytree=1.0, feature_fraction=0.616,\n",
       "              importance_type='split', lambda_l1=0, lambda_l2=0,\n",
       "              learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
       "              min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
       "              n_jobs=-1, num_leaves=50, objective=None, random_state=None,\n",
       "              reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "              subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lgb_reg.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
